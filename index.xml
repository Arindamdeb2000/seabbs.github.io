<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sam Abbott on Sam Abbott</title>
    <link>http://www.samabbott.co.uk/</link>
    <description>Recent content in Sam Abbott on Sam Abbott</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; 2017-2019 Sam Abbott</copyright>
    <lastBuildDate>Tue, 03 Sep 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>getTBinR 0.7.0 released - more data, {ggplot2} best practices and bug fixes</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-7-0/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-7-0/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR 0.7.0&lt;/code&gt;&lt;/a&gt; should now be available on CRAN. This release includes some new experimental data (TB incidence by age and sex) that for now is only partly supported by &lt;code&gt;{getTBinR}&lt;/code&gt;. It also brings &lt;code&gt;{getTBinR}&lt;/code&gt; into line with new (or new to me) &lt;code&gt;{ggplot2}&lt;/code&gt; &lt;a href=&#34;https://ggplot2.tidyverse.org/dev/articles/ggplot2-in-packages.html&#34;&gt;best practices&lt;/a&gt;. This involved two major changes (plans are also afoot for an S3 &lt;code&gt;plot&lt;/code&gt; method):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Moving from a full &lt;code&gt;@import&lt;/code&gt; of &lt;code&gt;{ggplot2}&lt;/code&gt; to only using &lt;code&gt;@importFrom&lt;/code&gt; for required functions (something that I had previously been too lazy to do…).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dropping &lt;code&gt;aes_string&lt;/code&gt;, which is now soft depreciated, in favour of &lt;code&gt;aes&lt;/code&gt; and &lt;code&gt;rlang::.data&lt;/code&gt; for programmatic variables (Note: &lt;em&gt;Yes I could have more fully embraced &lt;code&gt;rlang&lt;/code&gt; here but this would have required some major breaking package changes&lt;/em&gt;). This was a real joy as &lt;code&gt;aes_string&lt;/code&gt; has always been a bit clunky to use and resulted in very messy looking code. A simple pseudo-code example of this can be seen below.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

## Variable to plot
y_var &amp;lt;- &amp;quot;disp&amp;quot;

## Old getTBinR approach for programming across variables
ggplot(mtcars, aes_string(x = &amp;quot;cyl&amp;quot;, y = y_var)) + 
  geom_point()

## New getTBinR approach using rlang
ggplot(mtcars, aes(x = cyl, y = .data[[y_var]])) + 
  geom_point() +
  ## This is now needed to get the correct label - a small price to pay.
  labs(y = y_var)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wrapping up this release are several bug fixes for the embedded &lt;code&gt;{shiny}&lt;/code&gt; app and &lt;code&gt;{rmarkdown}&lt;/code&gt; report. Hopefully, &lt;code&gt;{getTBinR}&lt;/code&gt; is now ready to be presented next week at &lt;a href=&#34;https://r-medicine.com&#34;&gt;R medicine&lt;/a&gt; (see your there)! The full change log is included below (or can be found &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/news/index.html&#34;&gt;here&lt;/a&gt;). See the bottom of this post for an example of using the new age and sex stratified incidence data.&lt;/p&gt;
&lt;div id=&#34;gettbinr-0.7.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;getTBinR 0.7.0&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added experimental support for incidence data stratified by age and sex. Current implementation requires data cleaning before use. See the release post for details.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;package-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fixed a bug that was preventing &lt;code&gt;render_country_report&lt;/code&gt; from producing a country level report. Added tests to flag this in the future.&lt;/li&gt;
&lt;li&gt;Updated the packages requested for installation by &lt;code&gt;run_tb_dashboard&lt;/code&gt; so that &lt;code&gt;render_country_report&lt;/code&gt; runs without errors.&lt;/li&gt;
&lt;li&gt;Switched to using &lt;code&gt;ggplot2&lt;/code&gt; best practices (&lt;a href=&#34;https://github.com/seabbs/getTBinR/issues/77&#34;&gt;#77&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Updated the README to make identifying types of badges easier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-of-tb-incidence-in-each-age-group&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: % of TB incidence in each age group&lt;/h2&gt;
&lt;p&gt;The code below downloads the new experimental (and in its raw form messy) incidence data stratified by age and sex. As these data are in a different format to the rest of the data in &lt;code&gt;{getTBinR}&lt;/code&gt; some cleaning + aggregation is needed to get it into a workable form. From there it is a simple task to use &lt;code&gt;{getTBinR}&lt;/code&gt; to produce a global map of the % of TB incidence in each age group. For interest I have also used &lt;code&gt;{ggridges}&lt;/code&gt; to look at the regional distribution of TB incidence stratified by age group. Finally everything is pulled together into a single figure using &lt;code&gt;{patchwork}&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load_gh(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;tidyr&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load(&amp;quot;patchwork&amp;quot;)

##Pull TB data 
tb_burden &amp;lt;- get_tb_burden(additional_datasets = &amp;quot;Incidence by age and sex&amp;quot;,
                           verbose = FALSE) %&amp;gt;% 
  mutate(country = country %&amp;gt;% 
           factor()) %&amp;gt;% 
  mutate(age_group = age_group %&amp;gt;% 
           factor(levels = c(&amp;quot;0-4&amp;quot;, paste(seq(5, 55, 10), seq(14, 64, 10), sep = &amp;quot;-&amp;quot;), &amp;quot;65plus&amp;quot;)) %&amp;gt;% 
         fct_explicit_na()) 

## Aggregate incidence by age group
tb_burden_ag &amp;lt;- tb_burden %&amp;gt;%
filter(age_group != &amp;quot;0-14&amp;quot;) %&amp;gt;% 
mutate(age_group = age_group %&amp;gt;% 
         fct_drop()) %&amp;gt;% 
group_by(age_group, country, .drop = FALSE) %&amp;gt;%
summarise_at(.vars = vars(inc_age_sex, inc_age_sex_lo, inc_age_sex_hi), sum) %&amp;gt;%
full_join(
  select(tb_burden, -sex, -age_group, -contains(&amp;quot;inc_age_sex&amp;quot;)) %&amp;gt;%
    unique
  ) %&amp;gt;% 
ungroup %&amp;gt;% 
  add_count(country, year, wt = inc_age_sex) %&amp;gt;% 
  mutate(inc_age_dist = inc_age_sex / n * 100) %&amp;gt;% 
  filter(!(age_group %in% &amp;quot;(Missing)&amp;quot;))


## Map using getTBinR
map &amp;lt;- map_tb_burden(tb_burden_ag, metric = &amp;quot;inc_age_dist&amp;quot;, 
                     metric_label = &amp;quot;% of TB incidence in each age group&amp;quot;,
                     facet = c(&amp;quot;age_group&amp;quot;)) +
  labs(title = &amp;quot;Distribution of Tuberculosis (TB) incidence by age&amp;quot;,
       subtitle = &amp;quot;Global map of the % of TB incidence in each age group&amp;quot;,
       caption = &amp;quot;&amp;quot;) +
  theme(plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = 26), text = element_text(size = 20))

## Density plot using ggridges
density &amp;lt;- tb_burden_ag %&amp;gt;% 
drop_na(inc_age_dist) %&amp;gt;% 
mutate(age_group = age_group %&amp;gt;% 
         fct_rev()) %&amp;gt;% 
ggplot(aes(x = inc_age_dist, y = age_group, col = age_group, fill = age_group)) +
  geom_density_ridges(alpha = 0.8) +
  facet_wrap(~g_whoregion) +
  theme_ridges() +
  theme(legend.position = &amp;quot;none&amp;quot;, plot.subtitle = element_text(size = 20),
        plot.title = element_text(size = 20), plot.caption = element_text(size = 20),
        text = element_text(size = 20)) +
  labs(title = &amp;quot;Regional differences in the % of TB incidence in each age group&amp;quot;,
       subtitle = &amp;quot;+ within country differences for each region&amp;quot;,
       x = &amp;quot;Age group&amp;quot;, 
       y = &amp;quot;% of TB incidence&amp;quot;,
       caption = &amp;quot;By @seabbs | Made with {getTBinR} | Source: World Health Organization&amp;quot;) +
  scale_fill_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;) 

## Patchwork storyboard
storyboard &amp;lt;- (map) /
  (density)

## Save everything
ggsave(&amp;quot;../../static/img/getTBinR/storyboard-7-0.png&amp;quot;,
       storyboard, width = 32, height = 32, dpi = 330)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/getTBinR/storyboard-7-0.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;. To start using &lt;code&gt;getTBinR&lt;/code&gt; in your browser see &lt;a href=&#34;https://mybinder.org/v2/gh/seabbs/getTBinR/master?urlpath=rstudio&#34;&gt;here&lt;/a&gt; for an Rstudio client hosted by &lt;a href=&#34;https://mybinder.org&#34;&gt;mybinder.org&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modelling BCG vaccination in the UK: What is the impact of changing policy? (PhD thesis)</title>
      <link>http://www.samabbott.co.uk/project/thesis/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring the effects of BCG vaccination in patients diagnosed with tuberculosis: observational study using the Enhanced Tuberculosis Surveillance system</title>
      <link>http://www.samabbott.co.uk/publication/explorebcgonoutcomes/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/publication/explorebcgonoutcomes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Celebrating £1.4bn for the Global Fund - Trying out gganimate on Tuberculosis data from getTBinR</title>
      <link>http://www.samabbott.co.uk/post/tb-gifs/</link>
      <pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/tb-gifs/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.bbc.co.uk/news/uk-48810060?intlink_from_url=https://www.bbc.co.uk/news/topics/cmj34zmwxx1t/tuberculosis&amp;amp;link_location=live-reporting-story&#34;&gt;Last week&lt;/a&gt; the UK pledged to contribute £467m a year for three years to the Global Fund. The money will be spent on: providing tuberculosis (TB) treatment for more than two million people; 90 million mosquito nets to protect people from malaria; and treatment for more than three million people living with HIV. This funding will drastically improve many peoples lives and needs to be celebrated even if it comes from a broadly unpopular source.&lt;/p&gt;
&lt;p&gt;Whilst the Global Fund has helped &lt;a href=&#34;https://www.gov.uk/government/news/pm-calls-on-g20-leaders-to-step-up-the-fight-against-deadly-diseases?sf104862388=1&#34;&gt;save more than 27 million lives&lt;/a&gt; since 2002 all three diseases continue to have a devastating impact with TB remaining as one of the top ten causes of death worldwide, and the number one cause of death from an infectious disease. More research and funding is needed to continue to reduce the incidence, and negative consequences, of these diseases.&lt;/p&gt;
&lt;p&gt;In this post, I will be celebrating this funding by repeating/adapting some previous work visualising trends in Tuberculosis (TB) incidence rates but now using GIFs. The starting point for this post is the storyboard below (from a &lt;a href=&#34;https://www.samabbott.co.uk/post/gettbinr-6-0/&#34;&gt;previous post&lt;/a&gt;). Using the code for each plot I will work through animating them using &lt;a href=&#34;https://gganimate.com/index.html&#34;&gt;&lt;code&gt;gganimate&lt;/code&gt;&lt;/a&gt; (by &lt;a href=&#34;https://github.com/thomasp85&#34;&gt;Thomas Lin Pedersen&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/getTBinR/storyboard-6-0.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This storyboard shows the trends in regional/global TB incidence rates, as well as the annual percentage change in incidence rates, both globally, and on a country level (ridge plots). &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt;&lt;/a&gt; was used to produce all of the figures excepting the ridge plots which were generated using &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;&lt;code&gt;ggridges&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-6-0.png&#34;&gt;here&lt;/a&gt; for a full size version.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gganimate&lt;/code&gt; was recently reworked to be a complimentary grammar to that used in &lt;code&gt;ggplot&lt;/code&gt; - rather than as an additional dimension of it. The rework comes with some interesting new features - the &lt;a href=&#34;https://www.data-imaginist.com/2019/gganimate-has-transitioned-to-a-state-of-release/&#34;&gt;release post&lt;/a&gt; is well worth a read! Another motivation for this post is to explore some of these features.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;Here we use &lt;code&gt;pacman&lt;/code&gt; to handle downloading and loading the required &lt;code&gt;rstats&lt;/code&gt; packages. See the comments below for external dependencies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;tidyverse&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;gifski&amp;quot;) ##requires cargo on ubuntu
p_load(&amp;quot;magick&amp;quot;) ## requires libmagick++-dev on ubuntu
p_load_gh(&amp;quot;thomasp85/gganimate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;getTbinR&lt;/code&gt; handles downloading and cleaning the TB surveillance data provided by the World Health Organization. See &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/dev/reference/get_tb_burden.html&#34;&gt;here&lt;/a&gt; for details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Pull TB data 
tb_burden &amp;lt;- get_tb_burden(verbose = FALSE) 

tb_burden&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,850 x 68
##    country iso2  iso3  iso_numeric g_whoregion  year e_pop_num e_inc_100k
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 Afghan… AF    AFG             4 Eastern Me…  2000  20093756        190
##  2 Afghan… AF    AFG             4 Eastern Me…  2001  20966463        189
##  3 Afghan… AF    AFG             4 Eastern Me…  2002  21979923        189
##  4 Afghan… AF    AFG             4 Eastern Me…  2003  23064851        189
##  5 Afghan… AF    AFG             4 Eastern Me…  2004  24118979        189
##  6 Afghan… AF    AFG             4 Eastern Me…  2005  25070798        189
##  7 Afghan… AF    AFG             4 Eastern Me…  2006  25893450        189
##  8 Afghan… AF    AFG             4 Eastern Me…  2007  26616792        189
##  9 Afghan… AF    AFG             4 Eastern Me…  2008  27294031        189
## 10 Afghan… AF    AFG             4 Eastern Me…  2009  28004331        189
## # … with 3,840 more rows, and 60 more variables: e_inc_100k_lo &amp;lt;dbl&amp;gt;,
## #   e_inc_100k_hi &amp;lt;dbl&amp;gt;, e_inc_num &amp;lt;int&amp;gt;, e_inc_num_lo &amp;lt;int&amp;gt;,
## #   e_inc_num_hi &amp;lt;int&amp;gt;, e_tbhiv_prct &amp;lt;dbl&amp;gt;, e_tbhiv_prct_lo &amp;lt;dbl&amp;gt;,
## #   e_tbhiv_prct_hi &amp;lt;dbl&amp;gt;, e_inc_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_inc_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_inc_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_inc_tbhiv_num &amp;lt;int&amp;gt;, e_inc_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_inc_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_exc_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_mort_exc_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_mort_exc_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_mort_exc_tbhiv_num &amp;lt;int&amp;gt;, e_mort_exc_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_exc_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_mort_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_mort_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_mort_tbhiv_num &amp;lt;int&amp;gt;, e_mort_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_100k &amp;lt;dbl&amp;gt;, e_mort_100k_lo &amp;lt;dbl&amp;gt;,
## #   e_mort_100k_hi &amp;lt;dbl&amp;gt;, e_mort_num &amp;lt;int&amp;gt;, e_mort_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_num_hi &amp;lt;int&amp;gt;, cfr &amp;lt;dbl&amp;gt;, cfr_lo &amp;lt;dbl&amp;gt;, cfr_hi &amp;lt;dbl&amp;gt;,
## #   c_newinc_100k &amp;lt;dbl&amp;gt;, c_cdr &amp;lt;dbl&amp;gt;, c_cdr_lo &amp;lt;dbl&amp;gt;, c_cdr_hi &amp;lt;dbl&amp;gt;,
## #   source_rr_new &amp;lt;chr&amp;gt;, source_drs_coverage_new &amp;lt;chr&amp;gt;,
## #   source_drs_year_new &amp;lt;int&amp;gt;, e_rr_pct_new &amp;lt;dbl&amp;gt;, e_rr_pct_new_lo &amp;lt;dbl&amp;gt;,
## #   e_rr_pct_new_hi &amp;lt;dbl&amp;gt;, e_mdr_pct_rr_new &amp;lt;int&amp;gt;, source_rr_ret &amp;lt;chr&amp;gt;,
## #   source_drs_coverage_ret &amp;lt;chr&amp;gt;, source_drs_year_ret &amp;lt;int&amp;gt;,
## #   e_rr_pct_ret &amp;lt;dbl&amp;gt;, e_rr_pct_ret_lo &amp;lt;dbl&amp;gt;, e_rr_pct_ret_hi &amp;lt;dbl&amp;gt;,
## #   e_mdr_pct_rr_ret &amp;lt;int&amp;gt;, e_inc_rr_num &amp;lt;int&amp;gt;, e_inc_rr_num_lo &amp;lt;int&amp;gt;,
## #   e_inc_rr_num_hi &amp;lt;int&amp;gt;, e_mdr_pct_rr &amp;lt;int&amp;gt;,
## #   e_rr_in_notified_pulm &amp;lt;int&amp;gt;, e_rr_in_notified_pulm_lo &amp;lt;int&amp;gt;,
## #   e_rr_in_notified_pulm_hi &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Pull the data dictionarty
tb_dict &amp;lt;- get_data_dict(verbose = FALSE)

tb_dict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 470 x 4
##    variable_name  dataset code_list definition                             
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                                  
##  1 budget_cpp_ds… Budget  &amp;quot;&amp;quot;        Average cost of drugs budgeted per pat…
##  2 budget_cpp_mdr Budget  &amp;quot;&amp;quot;        Average cost of drugs budgeted per pat…
##  3 budget_cpp_xdr Budget  &amp;quot;&amp;quot;        Average cost of drugs budgeted per pat…
##  4 budget_fld     Budget  &amp;quot;&amp;quot;        Budget required for drugs to treat dru…
##  5 budget_lab     Budget  &amp;quot;&amp;quot;        Budget required for laboratory infrast…
##  6 budget_mdrmgt  Budget  &amp;quot;&amp;quot;        Budget required for programme costs to…
##  7 budget_orsrvy  Budget  &amp;quot;&amp;quot;        Budget required for operational resear…
##  8 budget_oth     Budget  &amp;quot;&amp;quot;        Budget required for all other budget l…
##  9 budget_patsup  Budget  &amp;quot;&amp;quot;        Budget required for patient support (U…
## 10 budget_prog    Budget  &amp;quot;&amp;quot;        Budget required for programme costs to…
## # … with 460 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first step (though not the most exciting) is to define what size all the gifs in this post should be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gif_width &amp;lt;- 720
gif_height &amp;lt;- 720&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;regional-tb-incidence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regional TB incidence rates&lt;/h2&gt;
&lt;p&gt;The code below uses some of the built-in in functionality from &lt;code&gt;getTBinR&lt;/code&gt; to generate a summary figure showing trends in regional TB incidence rates. &lt;code&gt;gganimate::transition_reveal&lt;/code&gt; is then used to make this &lt;code&gt;ggplot&lt;/code&gt; into a GIF that reveals a data point at a time. The &lt;code&gt;gganimate::transition_&lt;/code&gt; family of functions interpret the plot data in order to somehow distribute it over a number of frames. See the &lt;a href=&#34;https://gganimate.com/reference/index.html&#34;&gt;&lt;code&gt;gganimate&lt;/code&gt; documentation&lt;/a&gt; for the full list of transitions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Regional and Global TB incidence rates over time
regional_incidence &amp;lt;- plot_tb_burden_summary(conf = NULL, compare_to_world = FALSE) +
    labs(title = &amp;quot;Tuberculosis (TB) Incidence Rates&amp;quot;,
         subtitle = &amp;quot;By Region&amp;quot;,
         caption = &amp;quot;&amp;quot;) +
  facet_wrap(~Area, scales = &amp;quot;free_y&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;, plot.title = element_text(size=22)) +
  ##gganimate code
  transition_reveal(year)

## Animate
regional_incidence_gif &amp;lt;- animate(regional_incidence, 
                                  width = gif_width,
                                  height = gif_height)

## Print
regional_incidence_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-07-04-tb-gifs_files/figure-html/plot-tb-summary-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summarise-the-annual-percentage-change-in-tb-incidence-rates.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summarise the annual percentage change in TB incidence rates.&lt;/h2&gt;
&lt;p&gt;We again use some of the functionality from &lt;code&gt;getTbinR&lt;/code&gt; to estimate the annual percentage in TB incidence rates globally.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Summarise global changes
global_tb &amp;lt;- summarise_tb_burden(compare_to_world = TRUE,
                                 annual_change = TRUE,
                                 stat = &amp;quot;rate&amp;quot;,
                                 verbose = FALSE) %&amp;gt;% 
  filter(area == &amp;quot;Global&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;ggplot&lt;/code&gt; is used to produce a basic line plot and then &lt;code&gt;gganimate&lt;/code&gt; is used to turn this into a GIF (as above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Global annual change
global_annual_change &amp;lt;- ggplot(global_tb, aes(year, e_inc_num)) +
  geom_line(col = &amp;quot;black&amp;quot;, size = 1.4, alpha = 0.6) +
  ## geom_point needs a year based group so that it renders correctly in the gif
  geom_point(size = 1.6, alpha = 1, col = &amp;quot;black&amp;quot;, aes(group = seq_along(year))) + 
  scale_y_continuous(label = scales::percent, minor_breaks = NULL, breaks = seq(-0.025, 0, 0.0025)) +
  theme_minimal() +
  labs(
    y = &amp;quot;Annual % Change&amp;quot;,
    x = &amp;quot;Year&amp;quot;,
    title = &amp;quot;Global Annual % Change in Tuberculosis Incidence Rates&amp;quot;,
    caption = &amp;quot;&amp;quot;
  ) + 
  ##gganimate code
  transition_reveal(year)

## Animate the gif
global_annual_change_gif &amp;lt;- animate(global_annual_change, width = gif_width, height = gif_height)

## Print
global_annual_change_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-07-04-tb-gifs_files/figure-html/annual-change-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;country-level-annual-percentage-change-in-tb-incidence-rates.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Country level annual percentage change in TB incidence rates.&lt;/h2&gt;
&lt;p&gt;This repeats the steps from above but rather than summarising everything at a global level keeps the country level data. Countries with low incidence rates (&amp;lt; 10 per 100,000) or with very few cases (&amp;lt; 1000) have been filtered out to reduce the noise in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Remove countries with incidence below 1000 or incidence rates below 10 per 100,000 to reduce noise and cal country level annual change.
countries_with_tb_burden &amp;lt;- tb_burden %&amp;gt;% 
  filter(year == 2017,
         e_inc_100k &amp;gt; 10,
         e_inc_num &amp;gt; 1000) %&amp;gt;% 
  pull(country)

tb_annual_change &amp;lt;- summarise_tb_burden(countries = countries_with_tb_burden, 
                                        compare_to_region = FALSE, 
                                        compare_to_world = FALSE,
                                        metric = &amp;quot;e_inc_100k&amp;quot;,
                                        annual_change = TRUE,
                                        stat = &amp;quot;mean&amp;quot;,
                                        verbose = FALSE) %&amp;gt;% 
  mutate(annual_change = e_inc_100k) %&amp;gt;% 
  left_join(tb_burden %&amp;gt;% 
              select(country, g_whoregion), 
                     by = c(&amp;quot;area&amp;quot; = &amp;quot;country&amp;quot;)) %&amp;gt;% 
  drop_na(g_whoregion)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now define a density plotting function for this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Function to plot annual change
plot_annual_change &amp;lt;- function(df, strat = NULL, title = NULL, subtitle = NULL, years = 2000:2017) {
  dist &amp;lt;- df %&amp;gt;% 
    filter(year %in% years) %&amp;gt;% 
    rename(Region = g_whoregion) %&amp;gt;% 
    mutate(year = year %&amp;gt;% 
             factor(ordered = TRUE) %&amp;gt;% 
             fct_rev) %&amp;gt;% 
    ggplot(aes_string(x = &amp;quot;annual_change&amp;quot;, fill = strat)) +
    geom_density(alpha = 0.6) +
    scale_color_viridis(discrete = TRUE, end = 0.9) +
    scale_fill_viridis(discrete = TRUE, end = 0.9) +
    geom_vline(xintercept = 0, linetype = 2, alpha = 0.6) +
    scale_x_continuous(labels = scales::percent, breaks = seq(-0.4, 0.4, 0.1),
                       limits = c(-0.4, 0.4), minor_breaks = NULL) +
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(x = paste0(&amp;quot;Annual Change in &amp;quot;, search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition),
         y = &amp;quot;Density&amp;quot;,
         title = title,
         subtitle = subtitle,
         caption = &amp;quot;&amp;quot;)
  
  return(dist)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;plot_annual_change&lt;/code&gt; we can now generate first a &lt;code&gt;ggplot&lt;/code&gt; density of plot of the annual percentage change in countries TB incidence rates. This is again transformed into a GIF using &lt;code&gt;gganimate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Rather than using &lt;code&gt;gganimate::transition_reveal&lt;/code&gt; as before, here we use &lt;code&gt;gganimate::transition_states&lt;/code&gt; followed by &lt;code&gt;gganimate::shadow_trail&lt;/code&gt;. Also, rather than revealing a data point each time, we plot a single density plot for each group instead (here that grouping is by year) and leave a shadow of the previous density plots. &lt;code&gt;{frame}&lt;/code&gt; has been used here to show which year is being plotted. Note we could have used &lt;code&gt;transition_time&lt;/code&gt; and &lt;code&gt;{frame_time}&lt;/code&gt; here but this was producing an error for me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;overall &amp;lt;- plot_annual_change(tb_annual_change, &amp;quot;&amp;#39;grey&amp;#39;&amp;quot;,
                   years = 2001:2017,
                   title = &amp;quot;Distribution of the Annual % Change in TB Incidence Rates by Country&amp;quot;,
                   subtitle = &amp;quot;Year: {2001 + round(16 * frame / nframes, 0)}&amp;quot;)  +
  scale_fill_manual(values = &amp;quot;grey&amp;quot;) + 
  ## gganimate code
  transition_states(year) +
  shadow_trail(alpha = alpha / 10, colour = &amp;quot;grey&amp;quot;)

## Animate
overall_gif &amp;lt;- animate(overall, width = gif_width, height = gif_height)

## Print
overall_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-07-04-tb-gifs_files/figure-html/unnamed-chunk-5-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;As above, this plots density of the annual percentage change in countries TB incidence rates but now stratified by region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region &amp;lt;- plot_annual_change(tb_annual_change, &amp;quot;Region&amp;quot;,
                             title = &amp;quot;Distribution of the Annual % Change in TB Incidence Rates by Country&amp;quot;,
                             subtitle = &amp;quot;By Region - Year: {2001 + round(16 * frame / nframes, 0)}&amp;quot;,
                             years = 2001:2017) + 
  facet_wrap(~Region, scales = &amp;quot;free_y&amp;quot;) + 
  labs(caption =  &amp;quot;@seabbs | Using #getTBinR and #gganimate | Data sourced from: World Health Organization&amp;quot;) + 
  ## gganimate code
  transition_states(year) +
  shadow_trail(alpha = alpha / 10, colour = &amp;quot;grey&amp;quot;)

## Animate
region_gif &amp;lt;- animate(region, width = gif_width, height = gif_height)

## Print
region_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-07-04-tb-gifs_files/figure-html/unnamed-chunk-6-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-a-storyboard-gif&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build a storyboard GIF&lt;/h2&gt;
&lt;p&gt;The code generates a storyboard GIF made up of all the GIFs in this post. It is based on a &lt;a href=&#34;https://github.com/thomasp85/gganimate/wiki/Animation-Composition&#34;&gt;great example&lt;/a&gt; by &lt;a href=&#34;https://github.com/CrumpLab&#34;&gt;Matt Crump&lt;/a&gt;, using &lt;code&gt;magick&lt;/code&gt; to attach the individual images in each GIF together. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-gif.gif&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard and &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-gif.png&#34;&gt;here&lt;/a&gt; for a static image of the end result&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Read in gif images
regional_incidence_mgif &amp;lt;- image_read(regional_incidence_gif)
global_annual_change_mgif &amp;lt;- image_read(global_annual_change_gif)


overall_mgif &amp;lt;- image_read(overall_gif)
region_mgif &amp;lt;- image_read(region_gif)


## Combine first image into a single storyboard
storyboard_gif &amp;lt;- image_append(c(
  image_append(c(regional_incidence_mgif[1], global_annual_change_mgif[1])), 
  image_append(c(overall_mgif[1], region_mgif[1]))),
  stack = TRUE
)

## Loop through each frame, compose a storyboard, and add to storyboard gif
for (i in 2:100) {
  storyboard_gif &amp;lt;- c(storyboard_gif, 
                      image_append(c(
  image_append(c(regional_incidence_mgif[i], global_annual_change_mgif[i])), 
  image_append(c(overall_mgif[i], region_mgif[i]))),
  stack = TRUE
))
}

## Save the gif - warning this takes around 10 minutes on my machine (https://www.samabbott.co.uk/post/building-an-rstats-workstation/)
anim_save(&amp;quot;../../static/img/getTBinR/storyboard-gif.gif&amp;quot;, storyboard_gif)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/getTBinR/storyboard-gif.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Hopefully this post was a fun way of celebrating the UK’s funding commitment to the Global Fund and provided some insights into how to make GIFs in &lt;code&gt;rstats&lt;/code&gt;. In terms of understanding the data I am not sure that using GIFs here offers much over the previous implementation but I am very happy to be proved wrong - let me know what you think. The map from the original storyboard was not reproduced as a GIF as there was no easy way to do so using the original code - watch this space.&lt;/p&gt;
&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;. To start using &lt;code&gt;getTBinR&lt;/code&gt; in your browser see &lt;a href=&#34;http://www.seabbs.co.uk/shiny/ExploreGlobalTB/&#34;&gt;here&lt;/a&gt; for a shiny app or &lt;a href=&#34;https://mybinder.org/v2/gh/seabbs/getTBinR/master?urlpath=rstudio&#34;&gt;here&lt;/a&gt; for an Rstudio client hosted by &lt;a href=&#34;https://mybinder.org&#34;&gt;mybinder.org&lt;/a&gt;. Tweet at &lt;a href=&#34;https://twitter.com/seabbs&#34;&gt;me&lt;/a&gt; if you use any of these resources to make something!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gganimate&lt;/code&gt; has a great &lt;a href=&#34;https://github.com/thomasp85/gganimate/wiki&#34;&gt;wiki&lt;/a&gt; with numerous other examples as well as a detailed &lt;a href=&#34;https://gganimate.com/articles/gganimate.html&#34;&gt;vignette&lt;/a&gt;. There is alot more functionality than that covered here so I recommended diving in with a dataset and seeing what you can create.&lt;/p&gt;
&lt;p&gt;If interested in TB research see &lt;a href=&#34;http://bit.ly/seabbs-google-scholar&#34;&gt;here&lt;/a&gt; for a list of my papers/preprints and &lt;a href=&#34;http://bit.ly/seabbs-code&#34;&gt;here&lt;/a&gt; for the corresponding, fully reproducible, code.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What do we really know about BCG</title>
      <link>http://www.samabbott.co.uk/talk/what-do-we-know-about-bcg-19-08-09/</link>
      <pubDate>Fri, 14 Jun 2019 14:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/talk/what-do-we-know-about-bcg-19-08-09/</guid>
      <description></description>
    </item>
    
    <item>
      <title>getTBinR 0.6.0 now on CRAN - from 80 variables to more than 450!</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-6-0/</link>
      <pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-6-0/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR 0.6.0&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update includes multiple new Tuberculosis datasets - increasing the available number of variables through &lt;code&gt;getTBinR&lt;/code&gt; from 80 to over 450. To help support these new datasets the package now contains a dataframe listing the available datasets and &lt;code&gt;search_data_dict&lt;/code&gt; can now also be used to search the data dictionary for variables by dataset. On top of this, this update contains suggested changes by reviewers (@&lt;a href=&#34;https://github.com/rrrlw&#34;&gt;rrrlw&lt;/a&gt; and @&lt;a href=&#34;https://github.com/strengejacke&#34;&gt;strengejacke&lt;/a&gt;) from &lt;a href=&#34;https://joss.theoj.org&#34;&gt;JOSS&lt;/a&gt; (see &lt;a href=&#34;https://github.com/openjournals/joss-reviews/issues/1260&#34;&gt;here&lt;/a&gt; for the review thread). If any package authors are reading this then I cannot recommend the JOSS review process enough!&lt;/p&gt;
&lt;p&gt;The full change log is included below (or can be found &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/news/index.html&#34;&gt;here&lt;/a&gt;). The JOSS paper can be found &lt;a href=&#34;https://doi.org/10.21105/joss.01260&#34;&gt;here&lt;/a&gt;. See the bottom of this post for an example of using the package to explore global Tuberculosis incidence rates.&lt;/p&gt;
&lt;div id=&#34;gettbinr-0.6.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;getTBinR 0.6.0&lt;/h2&gt;
&lt;div id=&#34;feature-updates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Feature updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Added a new &lt;code&gt;summarise_metric&lt;/code&gt; function to the package. This function was previously used internally by the TB report. For a given year it returns the value of a given metric, along with the regional and global rankings. The average change over the last decade is also supplied. Linked to #57.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;search_data_dict&lt;/code&gt; can now be used to search for a dataset by name. All variables in this dataset are then returned.&lt;/li&gt;
&lt;li&gt;Added a new dataframe, &lt;code&gt;available_datasets&lt;/code&gt;, that lists the datasets available to be imported into R using the package. This dataframe also gives a short description of each dataset, details the time-span of the dataset, and whether or not it is downloaded by default. Used by &lt;code&gt;get_tb_burden&lt;/code&gt; as a URL source for downloading the datasets. Linked to #58.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get_tb_burden&lt;/code&gt; can now import additional datasets (listed in &lt;code&gt;available_datasets&lt;/code&gt;), clean them, and then link them with the core dataset. This adds over 400 new variables to the package and provides a near complete list of data used in the WHO Tuberculosis global report. Please open an issue if you find an issue with this dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;package-updates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Package updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jumped to &lt;code&gt;0.6.0&lt;/code&gt; to signal a major release.&lt;/li&gt;
&lt;li&gt;Updated earliest supported R version based on travis testing - now &lt;code&gt;3.3.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Added the &lt;a href=&#34;https://doi.org/10.21105/joss.01260&#34;&gt;JOSS paper&lt;/a&gt; as the preferred citation for &lt;code&gt;getTBinR&lt;/code&gt; and also added this information to the README.&lt;/li&gt;
&lt;li&gt;URL and data save names have been deprecated from all functions and will be removed in a future release. This allows the number of arguments for many functions to be reduced with no loss of functionality (as data is only saved temporally by package functions).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;search_data_dict&lt;/code&gt; has improved messaging and no longer returns an error when nothing is found in the data dictionary. From #65.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;search_data_dict&lt;/code&gt; has expanded testing to account for new dataset searching and for failing to find results. Linked to #60.&lt;/li&gt;
&lt;li&gt;Dropped usage of &lt;code&gt;dplyr::funs&lt;/code&gt; as soft deprecated.&lt;/li&gt;
&lt;li&gt;Added tests for &lt;code&gt;summarise_metric&lt;/code&gt; and added to documentation.&lt;/li&gt;
&lt;li&gt;Added tests for additional dataset import in &lt;code&gt;get_tb_burden&lt;/code&gt;. #58&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;available_datasets&lt;/code&gt; and new data import to the README and to the getting started vignette.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gettbinr-0.5.8&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;getTBinR 0.5.8&lt;/h2&gt;
&lt;div id=&#34;package-updates-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Package updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Added package information to license file - suggested during review for JOSS submission by @&lt;a href=&#34;https://github.com/rrrlw&#34;&gt;rrrlw&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Updated README introduction to better explain package aim - suggested during review for JOSS submission by @&lt;a href=&#34;https://github.com/strengejacke&#34;&gt;strengejacke&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved package DESCRIPTION for CRAN only users - suggested during review for JOSS submission by @&lt;a href=&#34;https://github.com/rrrlw&#34;&gt;rrrlw&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;usethis::use_tidy_description&lt;/code&gt; to improve DESCRIPTION formatting.&lt;/li&gt;
&lt;li&gt;Added development documentation badge to the README + website.&lt;/li&gt;
&lt;li&gt;Moved to automated pkgdown deployment using travis. Based on &lt;a href=&#34;https://pkgdown.r-lib.org/reference/deploy_site_github.html&#34;&gt;this&lt;/a&gt; and the &lt;a href=&#34;https://github.com/tidyverse/dplyr/blob/master/.travis.yml&#34;&gt;dplyr&lt;/a&gt; implementation.&lt;/li&gt;
&lt;li&gt;Expanded travis testing grid based on &lt;a href=&#34;https://github.com/tidyverse/dplyr/blob/master/.travis.yml&#34;&gt;dplyr&lt;/a&gt; implementation.&lt;/li&gt;
&lt;li&gt;Updated earliest supported R version based on travis testing - now &lt;code&gt;3.2.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;usethis::use_tidy_versions()&lt;/code&gt; to set package to dependent on package versions used during development work. Added this to makefile to make automated.&lt;/li&gt;
&lt;li&gt;Added a git commit step to the &lt;code&gt;Makefile&lt;/code&gt; use with `make message=“your commit message”. This will automatically run all build steps that are required and then commit any changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-changes-in-tb-incidence-rates-in-2017&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Changes in TB incidence rates in 2017&lt;/h2&gt;
&lt;p&gt;The code below explores a subset of the available data by first estimating global incidence rates and the annual change between years. The country level annual changes in TB incidence rates are then plotted, first globally and then by region. Finally, the trend in incidence rates is explored using country, regional and global level TB incidence rates. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-6-0.png&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard.&lt;/p&gt;
&lt;p&gt;This example is an updated version of the example used for the &lt;a href=&#34;https://www.samabbott.co.uk/post/gettbinr-5-5/&#34;&gt;&lt;code&gt;0.5.5&lt;/code&gt; release blog post&lt;/a&gt;. It showcases some of the many improvements that have been made to the package since this release (see &lt;a href=&#34;https://gist.github.com/seabbs/5defcce1b90b3467318564b198ac33e5/revisions&#34;&gt;here&lt;/a&gt; for a comparison of the two examples)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;ggrepel&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;tidyr&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;)

##Pull TB data 
tb_burden &amp;lt;- get_tb_burden(verbose = FALSE) 


## Summarise global changes
global_tb &amp;lt;- summarise_tb_burden(compare_to_world = TRUE,
                                 annual_change = TRUE,
                                 stat = &amp;quot;rate&amp;quot;,
                                 verbose = FALSE) %&amp;gt;% 
  filter(area == &amp;quot;Global&amp;quot;)


## TB in 2017
tb_2017 &amp;lt;- global_tb %&amp;gt;% 
  filter(year == 2017)

## Global annual change
global_annual_change &amp;lt;- ggplot(global_tb, aes(year, e_inc_num)) +
  geom_smooth(se = FALSE, col = &amp;quot;black&amp;quot;, size = 1.2, alpha = 0.7) +
  geom_point(size = 1.2, alpha = 0.8, col = &amp;quot;black&amp;quot;) +
  scale_y_continuous(label = scales::percent, minor_breaks = NULL, breaks = seq(-0.025, 0, 0.0025)) +
  theme_minimal() +
  labs(
    y = &amp;quot;Annual Percentage Change&amp;quot;,
    x = &amp;quot;Year&amp;quot;,
    title = &amp;quot;Global Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
    caption = &amp;quot;&amp;quot;
  )

## Remove countries with incidence below 1000 or incidence rates below 10 per 100,000 to reduce noise and cal country level annual change.
countries_with_tb_burden &amp;lt;- tb_burden %&amp;gt;% 
  filter(year == 2017,
         e_inc_100k &amp;gt; 10,
         e_inc_num &amp;gt; 1000) %&amp;gt;% 
  pull(country)

tb_annual_change &amp;lt;- summarise_tb_burden(countries = countries_with_tb_burden, 
                                        compare_to_region = FALSE, 
                                        compare_to_world = FALSE,
                                        metric = &amp;quot;e_inc_100k&amp;quot;,
                                        annual_change = TRUE,
                                        stat = &amp;quot;mean&amp;quot;,
                                        verbose = FALSE) %&amp;gt;% 
  mutate(annual_change = e_inc_100k) %&amp;gt;% 
  left_join(tb_burden %&amp;gt;% 
              select(country, g_whoregion), 
                     by = c(&amp;quot;area&amp;quot; = &amp;quot;country&amp;quot;)) %&amp;gt;% 
  drop_na(g_whoregion)

## Function to plot annual change
plot_annual_change &amp;lt;- function(df, strat = NULL, subtitle = NULL, years = 2000:2017) {
  dist &amp;lt;- df %&amp;gt;% 
    filter(year %in% years) %&amp;gt;% 
    rename(Region = g_whoregion) %&amp;gt;% 
    mutate(year = year %&amp;gt;% 
             factor(ordered = TRUE) %&amp;gt;% 
             fct_rev) %&amp;gt;% 
    ggplot(aes_string(x = &amp;quot;annual_change&amp;quot;, y = &amp;quot;year&amp;quot;, col = strat, fill = strat)) +
    geom_density_ridges(quantile_lines = TRUE, quantiles = 2, alpha = 0.6) +
    scale_color_viridis(discrete = TRUE, end = 0.9) +
    scale_fill_viridis(discrete = TRUE, end = 0.9) +
    geom_vline(xintercept = 0, linetype = 2, alpha = 0.6) +
    scale_x_continuous(labels = scales::percent, breaks = seq(-0.4, 0.4, 0.1),
                       limits = c(-0.4, 0.4), minor_breaks = NULL) +
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(x = paste0(&amp;quot;Annual Change in &amp;quot;, search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition),
         y = &amp;quot;Year&amp;quot;,
         title = &amp;quot;Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
         subtitle = subtitle,
         caption = &amp;quot;&amp;quot;)
  
  return(dist)
}

## Overall country level annual change
overall &amp;lt;- plot_annual_change(tb_annual_change, NULL,
                              years = seq(2001, 2017, 2), subtitle = &amp;quot;By Country&amp;quot;) 


## Regional country level annual change
region &amp;lt;-  plot_annual_change(tb_annual_change, &amp;quot;Region&amp;quot;,
                              subtitle = &amp;quot;By Region&amp;quot;, 
                              years = seq(2001, 2017, 2)) + 
  facet_wrap(~Region) +
  labs(caption = &amp;quot;&amp;quot;)


## Regional and Global TB incidence rates over time
regional_incidence &amp;lt;- plot_tb_burden_summary(conf = NULL)

## Map global TB incidence rates for 2017 using getTBinR
map &amp;lt;- map_tb_burden(year = c(2005, 2009, 2013, 2017), facet = &amp;quot;year&amp;quot;) +
  theme(strip.background = element_blank()) +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
       subtitle = &amp;quot;By Country, per 100,000 population&amp;quot;)

## Compose storyboard
storyboard &amp;lt;- (map + regional_incidence + plot_layout(widths = c(2, 1))) /
  (region + (global_annual_change /
               overall + labs(caption = &amp;quot;For country level annual percentages change countries with incidence above 1000 and an incidence rate above 10 per 100,000 are shown.
                    The global annual percentage change is shown with a LOESS fit. 
                    By @seabbs | Made with getTBinR | Source: World Health Organisation&amp;quot;)) + plot_layout(widths = c(2, 1))) +
  plot_layout(widths = c(1, 1))

## Save storyboard
ggsave(&amp;quot;../../static/img/getTBinR/storyboard-6-0.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/getTBinR/storyboard-6-0.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;. To start using &lt;code&gt;getTBinR&lt;/code&gt; in your browser see &lt;a href=&#34;http://www.seabbs.co.uk/shiny/ExploreGlobalTB/&#34;&gt;here&lt;/a&gt; for a shiny app or &lt;a href=&#34;https://mybinder.org/v2/gh/seabbs/getTBinR/master?urlpath=rstudio&#34;&gt;here&lt;/a&gt; for an Rstudio client hosted by &lt;a href=&#34;https://mybinder.org&#34;&gt;mybinder.org&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reassessing the Evidence for Universal School-age Bacillus Calmette Guerin (BCG) Vaccination in England and Wales</title>
      <link>http://www.samabbott.co.uk/publication/assessbcgpolicychange/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/publication/assessbcgpolicychange/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating the effect of the 2005 change in BCG policy in England: A retrospective cohort study</title>
      <link>http://www.samabbott.co.uk/publication/directeffbcg/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/publication/directeffbcg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>getTBinR: an R package for accessing and summarising the World Health Organisation Tuberculosis data</title>
      <link>http://www.samabbott.co.uk/publication/gettbinr/</link>
      <pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/publication/gettbinr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benchmarking an Rstats workstation - using benchmarkme</title>
      <link>http://www.samabbott.co.uk/post/benchmarking-workstation-benchmarkme/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/benchmarking-workstation-benchmarkme/</guid>
      <description>


&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I recently &lt;a href=&#34;https://www.samabbott.co.uk/post/building-an-rstats-workstation/&#34;&gt;built out a new workstation&lt;/a&gt; and have done some &lt;a href=&#34;https://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/&#34;&gt;benchmarking using &lt;code&gt;xgboost&lt;/code&gt; via &lt;code&gt;h2o&lt;/code&gt;&lt;/a&gt;. In this post I am using the &lt;a href=&#34;https://www.jumpingrivers.com/blog/benchmarkme-new-version/&#34;&gt;&lt;code&gt;benchmarkme&lt;/code&gt;&lt;/a&gt; package to get another perspective on performance. &lt;em&gt;Note: The &lt;code&gt;benchmarkme&lt;/code&gt; package appears to have some issues when it comes to plotting benchmarks. I ended up having to drop them entirely from this post.&lt;/em&gt; &lt;strong&gt;Update (2019-02-11): Just checked this issue using &lt;code&gt;rocker/tidyverse:latest&lt;/code&gt; and found all &lt;code&gt;benchmarkme&lt;/code&gt; functionality is working well. Probably a user error on my part :) - testing code &lt;a href=&#34;https://gist.github.com/seabbs/13afd2ac53f96aeb937a7753af55fc56&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main thing that I am interested in checking is the performance trade-off of enabling simultaneous multithreading (i.e Hyper-threading for Intel CPUs). In the&lt;code&gt;xgboost&lt;/code&gt; benchmarks multithreading did nothing to improve performance - which reflected my everyday experience. This post represents a final check before turning it off and using the increased thermal overhead for additional real core overclocking.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;package-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package set-up&lt;/h2&gt;
&lt;p&gt;The code in this post is loosely based on that used in &lt;a href=&#34;https://www.jumpingrivers.com/blog/benchmarkme-new-version/&#34;&gt;this recent post&lt;/a&gt;. &lt;code&gt;pacman&lt;/code&gt; is used for package management and the &lt;code&gt;tidyverse&lt;/code&gt; is dragooned, as usual, for data munging and visualisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-benchmarks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Run benchmarks&lt;/h2&gt;
&lt;p&gt;The code below checks the system is discoverable and then runs the benchmarks of interest in a tidy manner.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get system details&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get Ram
get_ram()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 16.8 GB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get CPU
get_cpu()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $vendor_id
## [1] &amp;quot;GenuineIntel&amp;quot;
## 
## $model_name
## [1] &amp;quot;Intel(R) Core(TM) i7-5775R CPU @ 3.30GHz&amp;quot;
## 
## $no_of_cores
## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Run benchmarks&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Cached manually to avoid rerunning on knit/cache issue - does anyone have a nice package recommendation for this?
if (!file.exists(&amp;quot;../../static/data/workstation-benchmark/benchmarkme.rds&amp;quot;)) {
  benchmarks &amp;lt;- tibble(cores = c(1, 16, 32)) %&amp;gt;% 
  mutate(std = map(cores, ~ benchmark_std(cores = .), runs = 3, verbose = FALSE),
         io = map(cores, ~ benchmark_io(cores = .), runs = 3, verbose = FALSE))
  
  saveRDS(benchmarks, &amp;quot;../../static/data/workstation-benchmark/benchmarkme.rds&amp;quot;)
}else{
  benchmarks &amp;lt;- readRDS( &amp;quot;../../static/data/workstation-benchmark/benchmarkme.rds&amp;quot;)
}

benchmarks&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   cores std                    io                    
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;                 &amp;lt;list&amp;gt;                
## 1     1 &amp;lt;ben_results [45 × 6]&amp;gt; &amp;lt;ben_results [12 × 6]&amp;gt;
## 2    16 &amp;lt;ben_results [45 × 6]&amp;gt; &amp;lt;ben_results [12 × 6]&amp;gt;
## 3    32 &amp;lt;ben_results [45 × 6]&amp;gt; &amp;lt;ben_results [12 × 6]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note: It looks like resource use isn’t handled very cleanly in &lt;code&gt;benchmarkme&lt;/code&gt;. Looking at the screenshot below (fom the 16 core benchmark) we see that all cores are actually maxed out - including virtual cores - and there are multiple old jobs still active. Edit: This looks like it may have been user error - see the comment at the bottom of the post for details.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2019-02-05-benchmarking-workstation-benchmarkme/benchmarkme-htop.png&#34; alt=&#34;Load according to htop whilst running the 16 core test.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Load according to &lt;code&gt;htop&lt;/code&gt; whilst running the 16 core test.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualise&lt;/h2&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set-up&lt;/h3&gt;
&lt;p&gt;As I am really interested in comparing the performance of my system, rather than comparing it to other systems, I needed to develop a customised plot. The code below does this by taking the mean of each test, dividing by the single core run-time, and then plotting split by test type, the number of cores, and test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_benchmark &amp;lt;- function(df){
  df %&amp;gt;% 
  select(-cores1) %&amp;gt;%  
  mutate(elapsed_core_avg = elapsed / cores) %&amp;gt;% 
  group_by(test, cores, test_group) %&amp;gt;% 
  summarise(elapsed_core_avg  = mean(elapsed_core_avg , na.rm = TRUE)) %&amp;gt;% 
  group_by(test) %&amp;gt;% 
  arrange(cores) %&amp;gt;% 
  mutate(cores = factor(cores)) %&amp;gt;% 
  mutate(elapsed_over_max = elapsed_core_avg  / first(elapsed_core_avg)) %&amp;gt;% 
  mutate(Test = test) %&amp;gt;% 
  ggplot(aes(x = cores, y = elapsed_over_max, col  = Test, group = Test)) +
  geom_point(size = 1.2) +
  geom_line(size = 1.1, alpha = 0.6) + 
  theme_minimal() +
  facet_wrap(~ test_group, scales = &amp;quot;free_y&amp;quot;) + 
  scale_y_log10(labels = scales::percent) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  scale_color_viridis_d() +
  labs(x = &amp;quot;Cores&amp;quot;,
       y = &amp;quot;Time elapsed compared to single core performance (%)&amp;quot;,
       subtitle = &amp;quot;Stratified by test type&amp;quot;,
       caption = &amp;quot;Multicore performance is estimated by running the same test on each core and averaging the result. 
      This gives a biased estimate of performance as core set-up overhead is not included.&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-benchmark&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Programming benchmark&lt;/h3&gt;
&lt;p&gt;The impact of multiple cores appeared to vary depending on the test type. All matrix calculations and programming tests showed a clear speed up when using 16 cores over a single core but this speed-up varied between 70% and 85%. For matrix functions the picture was less clear with 3 tests performing worse when using 16 cores vs. a single core. In general, there appears to be, at most, a marginal improvement from using 32 cores (with 16 virtual) over 16 cores. However, it is likely that this does not represent the full picture as it does not seem that &lt;code&gt;benchmarkme&lt;/code&gt; includes multicore overheads (i.e from core set-up, merging results etc).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmarks %&amp;gt;% 
  unnest(std) %&amp;gt;% 
  plot_benchmark()+
  labs(title = &amp;quot;Performance benchmarks by core - programming&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-29-benchmarking-workstation-benchmarkme_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  ggsave(&amp;quot;../../static/img/2019-02-05-benchmarking-workstation-benchmarkme/cover_img.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;readwrite-benchmark&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Read/write benchmark&lt;/h3&gt;
&lt;p&gt;The impact of multiple cores is more consistently beneficial here, with each increase in the number of cores resulting in improved performance. This may be because the cores were not saturated by the processing workload so a linear speed up was possible. This performance improvement would likely increase as long as there was sufficient read/write bandwidth on the SSD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmarks %&amp;gt;% 
  unnest(io) %&amp;gt;% 
  plot_benchmark()+
  labs(title = &amp;quot;Performance benchmarks by core - data read/write&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-29-benchmarking-workstation-benchmarkme_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, I have explored multicore performance using the &lt;code&gt;benchmarkme&lt;/code&gt; package. I found that increasing the number of real cores improved performance in the majority of cases for both programming, and read/write tests. For programming tests, there was little evidence that using virtual cores led to a performance improvement but there was a clear benefit for read/write tests. This may be because the read/write tests did not use a complete CPU core of processing and so multiple jobs could be run at once. Unfortunately, these findings may not be that reliable as it appears that &lt;code&gt;benchmarkme&lt;/code&gt; tests multicore performance by running each test on each core. This ignores the overhead from setting up multiple cores, transferring data, and merging results. In some circumstances, this overhead can outweigh any benefit of using multiple cores.&lt;/p&gt;
&lt;p&gt;Based on the findings here, and from &lt;a href=&#34;https://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/&#34;&gt;my previous benchmarking post&lt;/a&gt;, I think it is fairly clear that most of the time virtual cores are really adding very little. When you consider that for nearly comparable performance double the RAM is required they appear even less attractive. Obviously I could keep benchmarking but I think it may be time to turn off multithreading, ramp up the CPU clock speed - BIOS fun here we come- and finally start getting some work done!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking an Rstats workstation on realistic workloads - using xgboost via h2o</title>
      <link>http://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/</guid>
      <description>


&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I recently &lt;a href=&#34;https://www.samabbott.co.uk/post/building-an-rstats-workstation/&#34;&gt;built out a new workstation&lt;/a&gt; to give me some local compute for data science workloads. Now that I have local access to both a CPU with a large number of cores (Threadripper 1950X with 16 cores) and a moderately powerful GPU (Nvidia RTX 2070), I’m interested in knowing when it is best to use CPU vs. GPU for some of the tasks that I commonly do.&lt;/p&gt;
&lt;p&gt;The first of these is fitting &lt;code&gt;xgboost&lt;/code&gt; models for prediction. This makes sense as a first problem to explore as in my experience, and in the experience of the wider community, &lt;code&gt;xgboost&lt;/code&gt; generally provides the best performance on tabular data - light GBM looks like it may be even better but the installation appears to be nightmarish - and predictive modelling is a fairly common use case. As I have recently been using the &lt;a href=&#34;https://www.h2o.ai&#34;&gt;&lt;code&gt;h2o&lt;/code&gt; package&lt;/a&gt; as my go-to tool, it makes sense for me to test &lt;code&gt;xgboost&lt;/code&gt; via &lt;code&gt;h2o&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I am also interested in exploring whether or not simultaneous multithreading (i.e Hyper-threading for Intel CPUs) gives any performance boost over using only physical cores for these workloads. I couldn’t find much on this online for AMD CPUs. My prior experience with Intel CPUs is that sticking to physical cores is the best option for nearly all serious compute. If this proves to be the case, disabling virtual core gives me a greater scope for overclocking!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;developing-a-testing-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Developing a testing function&lt;/h2&gt;
&lt;p&gt;To make this a relatively real-world test, I am going to be comparing run times on a grid of cross-validated models (10 models, with 5 folds each). A nice benefit of this is that we can also see the average performance of a configuration across a variety of hyper-parameters. In the code below I have specified the grid and used the &lt;code&gt;purrr:partial&lt;/code&gt; function to wrap everything up into a function. I’ve also turned off early stopping, which is not something that I would do in a real use case, to allow control over the exact number of trees being trained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Search criteria
search_crit &amp;lt;- list(strategy = &amp;quot;RandomDiscrete&amp;quot;, 
                    max_models = 10, stopping_rounds = 10)

hyper_params &amp;lt;- list(
  learn_rate = c(0.1, 0.3, 0.5),
  sample_rate = c(0.6, 0.8, 1),
  col_sample_rate = c(0.6, 0.8, 1),
  max_depth = c(1, 5, 10),
  min_rows = c(1, 2, 5),
  reg_lambda = c(0, 0.001),
  reg_alpha = c(0, 0.001)
)

spec_grid &amp;lt;- partial(h2o.grid,
                     algorithm = &amp;quot;xgboost&amp;quot;,
                     nfolds = 5,
                     seed = 2344232,
                     stopping_rounds = 0,
                     search_criteria = search_crit,
                     hyper_params = hyper_params)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to develop a function to fit and time a single grid. This needs to be specified by a subsample of the rows and columns, on a given number of CPU cores, and potentially with a GPU backend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_grid &amp;lt;- function(df,
                           target,
                           grid = NULL,
                           cores = NULL, 
                           gpu = FALSE,
                           rowsample = 1e3,
                           trees = NULL,
                           colsample = 1,
                           ram = 28) {
  
## Initialise the h2o cluster with the desired core number
  h2o.init(min_mem_size = paste0(ram, &amp;quot;g&amp;quot;),
           nthreads = cores)
  h2o.no_progress()

## Sample columns (up/down sampling)
  df &amp;lt;- df[target] %&amp;gt;% 
    bind_cols(df %&amp;gt;%
                select(-contains(target)) %&amp;gt;% 
                {.[, sample(1:ncol(.), colsample, replace = TRUE)]})
  
## Specify the training data and set 
  h2o_train &amp;lt;- sample_n(df, rowsample, replace = TRUE) %&amp;gt;% 
    as.h2o

## Specify the features
  features &amp;lt;- setdiff(colnames(df), target)
  
## Start the timer
tic(paste0(&amp;quot;Trained a &amp;quot;,
           &amp;quot;grid of 10 Xgboost &amp;quot;, 
           &amp;quot;models with &amp;quot;, cores, &amp;quot; cores&amp;quot;, 
            ifelse(gpu, &amp;quot; using the GPU backend&amp;quot;, &amp;quot;&amp;quot;),
            &amp;quot; on a subsample of &amp;quot;,
            rowsample, 
            &amp;quot; rows and &amp;quot;,
            colsample, 
           &amp;quot; features with &amp;quot;,
           trees, 
           &amp;quot; trees.&amp;quot;))

  if(object.size(df) &amp;gt; ((ram * 1000^3)/ cores)) {
    message(&amp;quot;Data size is to big to fit into RAM in this configuration&amp;quot;)
    
    model_fit &amp;lt;- FALSE
  }else{
    ## Train the models
  trained_grid &amp;lt;- grid(y = target,
                       x = features,
                       training_frame = h2o_train,
                       backend = ifelse(gpu, &amp;quot;gpu&amp;quot;, &amp;quot;cpu&amp;quot;)) 
  
  model_fit &amp;lt;- TRUE
  }


  
time &amp;lt;- toc()

time$fit &amp;lt;- model_fit

h2o.shutdown(prompt = FALSE)

Sys.sleep(3)
return(time)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sourcing-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sourcing test data&lt;/h2&gt;
&lt;p&gt;As the base for my testing data, I am using credit data from the &lt;code&gt;recipes&lt;/code&gt; package as an example of a real-world dataset. I went with a binary outcome as this reflects much of the modelling I have been doing day to day - usually loan defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credit_data &amp;lt;- recipes::credit_data %&amp;gt;% 
  as_tibble

skim(credit_data) %&amp;gt;% 
  skimr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Skim summary statistics&lt;br /&gt;
n obs: 4454&lt;br /&gt;
n variables: 14&lt;/p&gt;
&lt;p&gt;Variable type: factor&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;missing&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;complete&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;top_counts&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;ordered&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Home&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4448&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;own: 2107, ren: 973, par: 783, oth: 319&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Job&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4452&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;fix: 2805, fre: 1024, par: 452, oth: 171&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Marital&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4453&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;mar: 3241, sin: 977, sep: 130, wid: 67&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Records&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no: 3681, yes: 773, NA: 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Status&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;goo: 3200, bad: 1254, NA: 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Variable type: integer&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;missing&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;complete&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Age&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37.08&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10.98&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;68&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▅▇▇▇▅▃▂▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Amount&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1038.92&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;474.55&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;700&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1300&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▅▇▃▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Assets&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4407&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5403.98&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11574.42&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3e+05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▁▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Debt&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4436&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;343.03&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1245.99&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▁▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Expenses&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55.57&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19.52&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;180&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▃▃▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Income&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;381&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4073&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;141.69&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80.75&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;170&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;959&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▆▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Price&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1462.78&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;628.13&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1117.25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1691.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11140&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▆▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Seniority&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7.99&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.17&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▃▂▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Time&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46.44&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14.66&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▁▁▂▃▁▃▇▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This dataset has been cleaned and contains a limited number of, presumably fairly predictive, variables. To make this a more realistic test I’ve introduced additional numeric and categorical noise variables, as well as adding missing data and duplicating the original features - code below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add numeric and categorical noise features. Categorical features are randomly sampled and assigned 10, 50, 100, 250, 500 and 1000 levels, whilst numeric features are normally distributed with or without a log transform.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Set up categorical variable generation
get_cat_noise_var &amp;lt;- function(levels = NULL, samples) {
  sample(paste(&amp;#39;level&amp;#39;,1:levels,sep=&amp;#39;&amp;#39;), samples, replace=TRUE)
}

## Generate categorical variable with differing lengths (10, 100, 1000)
cat_noise_var &amp;lt;- map(c(10, 50, 100, 250, 500, 1000), ~ rep(., 5)) %&amp;gt;% 
  flatten %&amp;gt;% 
  map_dfc(~get_cat_noise_var(., nrow(credit_data))) %&amp;gt;% 
  set_names(paste0(&amp;quot;CatNoise_&amp;quot;, 1:30)) %&amp;gt;% 
  map_dfc(factor)

## Set up numeric variable generation. Normal with random mean and standard deviation (or log normal)
get_num_noise_var &amp;lt;- function(noise = 0.1, samples, log_shift = FALSE) {
  mean &amp;lt;- runif(1, -1e3, 1e3)
  x &amp;lt;- rnorm(samples, mean, abs(mean) * noise)
  
  if (log_shift) { 
   x &amp;lt;- log(abs(x + 1))
  }
  
  return(x)
}

## Generate numeric variables with varying amounts of noise and transforming using log
gen_numeric_var &amp;lt;- function(df, log_shift) {
  map(c(0.1, 0.2, 0.4), ~ rep(., 5)) %&amp;gt;% 
  flatten %&amp;gt;% 
  map_dfc( ~ get_num_noise_var(., nrow(df), log_shift))
}

num_noise_var &amp;lt;- gen_numeric_var(credit_data, log_shift = FALSE) %&amp;gt;% 
  bind_cols(gen_numeric_var(credit_data, log_shift = TRUE)) %&amp;gt;% 
    set_names(paste0(&amp;quot;NumNoise_&amp;quot;, 1:30))
  

## Bind together and summarise
noise_var &amp;lt;- cat_noise_var %&amp;gt;% 
  bind_cols(num_noise_var)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add duplicate informative features.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credit_data &amp;lt;- credit_data %&amp;gt;% 
  select(Status) %&amp;gt;% 
  bind_cols(credit_data %&amp;gt;% 
              select(-Status) %&amp;gt;% 
              {bind_cols(., .)} %&amp;gt;% 
              {bind_cols(., .)})&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add some missingness to the data and replace the homeowners &lt;code&gt;&amp;quot;other&amp;quot;&lt;/code&gt; category with 1000 random levels. Adding random noise levels to the homeowners variable means that some information is now encoded in a very noisy feature, providing more of a challenge for the &lt;code&gt;xgboost&lt;/code&gt; model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;add_miss &amp;lt;- function(x = NULL, max_miss = NULL) {
  miss_scale &amp;lt;- runif(1, 0, max_miss)
  
  x &amp;lt;- replace(x, runif(length(x), 0, 1) &amp;lt;= miss_scale, NA)
}



complex_credit_data &amp;lt;- credit_data %&amp;gt;% 
  bind_cols(noise_var) %&amp;gt;% 
  mutate_at(.vars = vars(everything(), - Status), ~ add_miss(., 0.2)) %&amp;gt;% 
  mutate(
    Home = case_when(Home %in% &amp;quot;other&amp;quot; ~ as.character(CatNoise_30),
                          TRUE ~ as.character(Home)) %&amp;gt;% 
           factor)


complex_credit_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,454 x 113
##    Status Seniority Home   Time   Age Marital Records Job   Expenses Income
##    &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;
##  1 good           9 rent     60    30 &amp;lt;NA&amp;gt;    no      free…       73    129
##  2 good          17 rent     60    58 widow   no      &amp;lt;NA&amp;gt;        48    131
##  3 bad           10 owner    36    46 married &amp;lt;NA&amp;gt;    free…       90    200
##  4 good           0 rent     60    24 single  no      fixed       63    182
##  5 good           0 rent     36    26 single  no      &amp;lt;NA&amp;gt;        46    107
##  6 good           1 owner    60    36 married no      &amp;lt;NA&amp;gt;        NA    214
##  7 good          29 owner    60    44 married no      fixed       75    125
##  8 good           9 pare…    12    27 &amp;lt;NA&amp;gt;    no      fixed       35     80
##  9 good           0 &amp;lt;NA&amp;gt;     60    32 married no      free…       90     NA
## 10 bad            0 pare…    48    41 married no      part…       90     80
## # … with 4,444 more rows, and 103 more variables: Assets &amp;lt;int&amp;gt;,
## #   Debt &amp;lt;int&amp;gt;, Amount &amp;lt;int&amp;gt;, Price &amp;lt;int&amp;gt;, Seniority1 &amp;lt;int&amp;gt;, Home1 &amp;lt;fct&amp;gt;,
## #   Time1 &amp;lt;int&amp;gt;, Age1 &amp;lt;int&amp;gt;, Marital1 &amp;lt;fct&amp;gt;, Records1 &amp;lt;fct&amp;gt;, Job1 &amp;lt;fct&amp;gt;,
## #   Expenses1 &amp;lt;int&amp;gt;, Income1 &amp;lt;int&amp;gt;, Assets1 &amp;lt;int&amp;gt;, Debt1 &amp;lt;int&amp;gt;,
## #   Amount1 &amp;lt;int&amp;gt;, Price1 &amp;lt;int&amp;gt;, Seniority2 &amp;lt;int&amp;gt;, Home2 &amp;lt;fct&amp;gt;,
## #   Time2 &amp;lt;int&amp;gt;, Age2 &amp;lt;int&amp;gt;, Marital2 &amp;lt;fct&amp;gt;, Records2 &amp;lt;fct&amp;gt;, Job2 &amp;lt;fct&amp;gt;,
## #   Expenses2 &amp;lt;int&amp;gt;, Income2 &amp;lt;int&amp;gt;, Assets2 &amp;lt;int&amp;gt;, Debt2 &amp;lt;int&amp;gt;,
## #   Amount2 &amp;lt;int&amp;gt;, Price2 &amp;lt;int&amp;gt;, Seniority11 &amp;lt;int&amp;gt;, Home11 &amp;lt;fct&amp;gt;,
## #   Time11 &amp;lt;int&amp;gt;, Age11 &amp;lt;int&amp;gt;, Marital11 &amp;lt;fct&amp;gt;, Records11 &amp;lt;fct&amp;gt;,
## #   Job11 &amp;lt;fct&amp;gt;, Expenses11 &amp;lt;int&amp;gt;, Income11 &amp;lt;int&amp;gt;, Assets11 &amp;lt;int&amp;gt;,
## #   Debt11 &amp;lt;int&amp;gt;, Amount11 &amp;lt;int&amp;gt;, Price11 &amp;lt;int&amp;gt;, CatNoise_1 &amp;lt;fct&amp;gt;,
## #   CatNoise_2 &amp;lt;fct&amp;gt;, CatNoise_3 &amp;lt;fct&amp;gt;, CatNoise_4 &amp;lt;fct&amp;gt;,
## #   CatNoise_5 &amp;lt;fct&amp;gt;, CatNoise_6 &amp;lt;fct&amp;gt;, CatNoise_7 &amp;lt;fct&amp;gt;,
## #   CatNoise_8 &amp;lt;fct&amp;gt;, CatNoise_9 &amp;lt;fct&amp;gt;, CatNoise_10 &amp;lt;fct&amp;gt;,
## #   CatNoise_11 &amp;lt;fct&amp;gt;, CatNoise_12 &amp;lt;fct&amp;gt;, CatNoise_13 &amp;lt;fct&amp;gt;,
## #   CatNoise_14 &amp;lt;fct&amp;gt;, CatNoise_15 &amp;lt;fct&amp;gt;, CatNoise_16 &amp;lt;fct&amp;gt;,
## #   CatNoise_17 &amp;lt;fct&amp;gt;, CatNoise_18 &amp;lt;fct&amp;gt;, CatNoise_19 &amp;lt;fct&amp;gt;,
## #   CatNoise_20 &amp;lt;fct&amp;gt;, CatNoise_21 &amp;lt;fct&amp;gt;, CatNoise_22 &amp;lt;fct&amp;gt;,
## #   CatNoise_23 &amp;lt;fct&amp;gt;, CatNoise_24 &amp;lt;fct&amp;gt;, CatNoise_25 &amp;lt;fct&amp;gt;,
## #   CatNoise_26 &amp;lt;fct&amp;gt;, CatNoise_27 &amp;lt;fct&amp;gt;, CatNoise_28 &amp;lt;fct&amp;gt;,
## #   CatNoise_29 &amp;lt;fct&amp;gt;, CatNoise_30 &amp;lt;fct&amp;gt;, NumNoise_1 &amp;lt;dbl&amp;gt;,
## #   NumNoise_2 &amp;lt;dbl&amp;gt;, NumNoise_3 &amp;lt;dbl&amp;gt;, NumNoise_4 &amp;lt;dbl&amp;gt;,
## #   NumNoise_5 &amp;lt;dbl&amp;gt;, NumNoise_6 &amp;lt;dbl&amp;gt;, NumNoise_7 &amp;lt;dbl&amp;gt;,
## #   NumNoise_8 &amp;lt;dbl&amp;gt;, NumNoise_9 &amp;lt;dbl&amp;gt;, NumNoise_10 &amp;lt;dbl&amp;gt;,
## #   NumNoise_11 &amp;lt;dbl&amp;gt;, NumNoise_12 &amp;lt;dbl&amp;gt;, NumNoise_13 &amp;lt;dbl&amp;gt;,
## #   NumNoise_14 &amp;lt;dbl&amp;gt;, NumNoise_15 &amp;lt;dbl&amp;gt;, NumNoise_16 &amp;lt;dbl&amp;gt;,
## #   NumNoise_17 &amp;lt;dbl&amp;gt;, NumNoise_18 &amp;lt;dbl&amp;gt;, NumNoise_19 &amp;lt;dbl&amp;gt;,
## #   NumNoise_20 &amp;lt;dbl&amp;gt;, NumNoise_21 &amp;lt;dbl&amp;gt;, NumNoise_22 &amp;lt;dbl&amp;gt;,
## #   NumNoise_23 &amp;lt;dbl&amp;gt;, NumNoise_24 &amp;lt;dbl&amp;gt;, NumNoise_25 &amp;lt;dbl&amp;gt;,
## #   NumNoise_26 &amp;lt;dbl&amp;gt;, NumNoise_27 &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-on-a-single-iteration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing on a single iteration&lt;/h2&gt;
&lt;p&gt;To check that everything is working as expected we test on a single iteration with 31 cores, no GPU, 1000 samples, 20 features and 50 trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test &amp;lt;- benchmark_grid(complex_credit_data,
                            &amp;quot;Status&amp;quot;,
                            grid = spec_grid,
                            cores = 31, 
                            gpu = FALSE,
                            rowsample = 1e4,
                            trees = 50,
                            colsample = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.out
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: . Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 213 milliseconds 
##     H2O cluster timezone:       Etc/UTC 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.23.0.4558 
##     H2O cluster version age:    5 days  
##     H2O cluster name:           H2O_started_from_R_seabbs_kha403 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   26.83 GB 
##     H2O cluster total cores:    32 
##     H2O cluster allowed cores:  31 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20) 
## 
## Trained a grid of 10 Xgboost models with 31 cores on a subsample of 10000 rows and 20 features with 50 trees.: 61.636 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $tic
## elapsed 
##   6.104 
## 
## $toc
## elapsed 
##   67.74 
## 
## $msg
## [1] &amp;quot;Trained a grid of 10 Xgboost models with 31 cores on a subsample of 10000 rows and 20 features with 50 trees.&amp;quot;
## 
## $fit
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the settings above give a runtime of around a minute but using the &lt;code&gt;htop&lt;/code&gt; tool we see that resource use is not stable over time. This may indicate that &lt;code&gt;h2o&lt;/code&gt; is not using all the supplied cores effectively/efficiently for this data size, with these settings etc.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2019-01-20-benchmarking-workstation-xgboost/load-example.gif&#34; alt=&#34;Load according to htop whilst running the test grid.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Load according to &lt;code&gt;htop&lt;/code&gt; whilst running the test grid.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;enabling-gpu-support&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enabling GPU support&lt;/h2&gt;
&lt;p&gt;Unlike using CPUs for &lt;code&gt;xgboost&lt;/code&gt;, enabling GPU support requires some extra steps (and lots of faff). As I have a Nvidia GPU, I need to install CUDA on my local machine (see &lt;a href=&#34;https://www.samabbott.co.uk/post/building-an-rstats-workstation/&#34;&gt;here&lt;/a&gt; for details); CUDA 8.0 (or higher) into the Docker container that this analysis is running in (see here for &lt;a href=&#34;https://github.com/seabbs/tidyverse-gpu&#34;&gt;the Dockerfile&lt;/a&gt; - thanks to &lt;a href=&#34;https://discuss.ropensci.org/t/tips-for-installing-cuda-into-a-rocker-docker-container/1556&#34;&gt;Noam Ross&lt;/a&gt; for the original implementation); and run the Docker container using the &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;Nvidia Docker runtime&lt;/a&gt;. To check everything is working, we run the same benchmark as above but now using the GPU.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test &amp;lt;- benchmark_grid(complex_credit_data,
                            &amp;quot;Status&amp;quot;,
                            grid = spec_grid,
                            cores = 31, 
                            gpu = TRUE,
                            rowsample = 1e4,
                            trees = 50,
                            colsample = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.out
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: . Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 86 milliseconds 
##     H2O cluster timezone:       Etc/UTC 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.23.0.4558 
##     H2O cluster version age:    5 days  
##     H2O cluster name:           H2O_started_from_R_seabbs_pon293 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   26.83 GB 
##     H2O cluster total cores:    32 
##     H2O cluster allowed cores:  31 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20) 
## 
## Trained a grid of 10 Xgboost models with 31 cores using the GPU backend on a subsample of 10000 rows and 20 features with 50 trees.: 236.803 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $tic
## elapsed 
##  73.641 
## 
## $toc
## elapsed 
## 310.444 
## 
## $msg
## [1] &amp;quot;Trained a grid of 10 Xgboost models with 31 cores using the GPU backend on a subsample of 10000 rows and 20 features with 50 trees.&amp;quot;
## 
## $fit
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Success! However, it has a much longer run time of nearly 4 minutes - not good. We again see (this time using the &lt;a href=&#34;https://github.com/Syllo/nvtop&#34;&gt;&lt;code&gt;nvtop&lt;/code&gt;&lt;/a&gt; tool) that resource use varies over time on the GPU.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2019-01-20-benchmarking-workstation-xgboost/load-example-gpu.gif&#34; alt=&#34;Load according to nvtop whilst running the test grid.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Load according to &lt;code&gt;nvtop&lt;/code&gt; whilst running the test grid.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;iterating-across-a-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iterating Across a Grid&lt;/h2&gt;
&lt;p&gt;Now that the timing function and the data are in place and everything is tested, I can run a full benchmarking grid. Using &lt;code&gt;expand.grid&lt;/code&gt;, I’ve combined all combinations of data sizes from 1,000 to 100,000 rows, from 10 to 1000 columns, from 10 to 10,000 trees and compute availability (here 4, 16, and 32 cores + GPU). Something that I have not implemented here, but that would reduce the noise in the final results, is running each benchmark multiple times. As you will see below, this is not feasible for a weekend blog post (or even the week or two blog post that this finally became!). &lt;em&gt;Note: I ended up dropping the 1000 feature combinations for the GPU as for deep trees (&lt;code&gt;max_depth&lt;/code&gt; = 10) I was getting out of memory errors.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grid set-up&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_input &amp;lt;- expand.grid(
  cores = c(4, 16, 32),
  rowsample = c(1e3, 1e4, 2.5e4, 5e4, 7.5e4, 1e5),
  colsample = c(10, 100, 1000),
  trees = c(10, 100, 1000, 10000),
  gpu = c(FALSE),
  rep = 1
) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  {bind_rows(., 
             filter(., cores == 4, colsample &amp;lt; 1000) %&amp;gt;% 
             mutate(gpu = TRUE))} %&amp;gt;% 
  mutate(size = rowsample * colsample * trees) %&amp;gt;% 
  arrange(desc(size), cores)

benchmark_input&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 264 x 7
##    cores rowsample colsample trees gpu     rep          size
##    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
##  1     4    100000      1000 10000 FALSE     1 1000000000000
##  2    16    100000      1000 10000 FALSE     1 1000000000000
##  3    32    100000      1000 10000 FALSE     1 1000000000000
##  4     4     75000      1000 10000 FALSE     1  750000000000
##  5    16     75000      1000 10000 FALSE     1  750000000000
##  6    32     75000      1000 10000 FALSE     1  750000000000
##  7     4     50000      1000 10000 FALSE     1  500000000000
##  8    16     50000      1000 10000 FALSE     1  500000000000
##  9    32     50000      1000 10000 FALSE     1  500000000000
## 10     4     25000      1000 10000 FALSE     1  250000000000
## # … with 254 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Run benchmark - making use of &lt;code&gt;tibble&lt;/code&gt; nesting and, the always slightly-hacky-feeling, &lt;code&gt;dplyr::rowwise&lt;/code&gt;. Everything here is crudely cached to avoid accidentlly overwriting results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Cached manually to avoid rerunning on knit.
if (!file.exists(&amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)) {
  benchmark_output_gpu &amp;lt;- benchmark_input %&amp;gt;% 
  rowwise() %&amp;gt;% 
  mutate(bench = list(as_tibble(benchmark_grid(complex_credit_data,
                                       &amp;quot;Status&amp;quot;,
                                        grid = spec_grid,
                                        cores = cores, 
                                        gpu = gpu,
                                        rowsample = rowsample,
                                        trees = trees,
                                        colsample = colsample)))) %&amp;gt;% 
  unnest(bench) %&amp;gt;%
  select(-msg) %&amp;gt;% 
  mutate(duration = toc - tic) %&amp;gt;% 
  filter(fit)
  
  saveRDS(benchmark_output, &amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)
}else{
  benchmark_output &amp;lt;- readRDS( &amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)
}

benchmark_output &amp;lt;- benchmark_output %&amp;gt;% 
  mutate(duration = duration / 60) %&amp;gt;% 
  arrange(gpu, cores) %&amp;gt;% 
  mutate(Compute = paste0(cores, &amp;quot; Threadripper 1950X CPU cores&amp;quot;) %&amp;gt;% 
           ifelse(gpu, &amp;quot;Nvidia 2070 GPU&amp;quot;, .))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;benchmarking-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Benchmarking Results&lt;/h2&gt;
&lt;p&gt;After leaving everything running for a few days, the results are in. The obvious plot to begin with is to split out everything by the number of trees and features and then plot duration against sample numbers for each compute amount (i.e cores and GPU).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_output %&amp;gt;% 
  mutate(Cores = factor(cores),
         GPU = gpu) %&amp;gt;% 
  ggplot(aes(rowsample, duration, col = Cores, shape = GPU, group = interaction(Cores, GPU))) +
  geom_point(size = 1.2) +
  geom_line(alpha = 0.8) + 
  facet_grid(colsample ~ trees, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, axis.text.x = element_text(angle = 90,hjust = 1)) +
  scale_x_continuous(labels = scales::comma) + 
  scale_color_viridis_d(end = 0.9, begin = 0.1) +
  labs(x = &amp;quot;Rows&amp;quot;,
       y = &amp;quot;Duration (minutes)&amp;quot;,
       caption = &amp;quot;Number of Trees ~ Number of Features&amp;quot;, 
       title = &amp;quot;Xgboost via h2o: Duration&amp;quot;,
       subtitle = &amp;quot;10 model hyper-parameter grid search with 5 fold cross-validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-26-benchmarking-workstation-xgboost_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the first major takeaway is that using the GPU appears to be slower, and mostly much slower, than using 4 CPU cores. This is very surprising to me as everything I have seen elsewhere would indicate that the GPU should offer some substantial speed up. There are some indications however that for larger data sets, and for larger tree numbers, the GPU may be comparable to multiple CPU cores. Potentially this is because any computational benefit from using the GPU is being swamped by the overhead of constantly passing data. Therefore, as the complexity of the problem increases so does the potential benefits of using the GPU. We see something similar for increasing the CPU count, with grids with 10 features running in nearly the same time for 4, 16 and 32 cores, whilst grids with 1000 features are drastically slower on 4 CPUs vs 16. Across all tests it looks like there is little benefit from using 32 (with 16 virtual) over 16 cores.&lt;/p&gt;
&lt;p&gt;To get a closer look at the CPU results and to try and understand the magnitude of the results, I’ve plotted the percentage improvement from a given compute amount over the longest duration for that number of rows - filtering out the GPU results as these would otherwise mask any other findings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_output %&amp;gt;% 
  filter(!gpu) %&amp;gt;% 
  group_by(rowsample, colsample, trees) %&amp;gt;% 
  mutate(duration = (max(duration) - duration) / max(duration)) %&amp;gt;% 
  mutate(Cores = factor(cores)) %&amp;gt;%
  ggplot(aes(rowsample, duration, duration, col = Cores)) +
  geom_point(size = 1.2) +
  geom_line(alpha = 0.8) + 
  facet_grid(colsample ~ trees, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, axis.text.x = element_text(angle=90,hjust=1)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::comma) + 
  scale_color_viridis_d(begin = 0.2, end = 0.8) +
  labs(x = &amp;quot;Rows&amp;quot;,
       y = &amp;quot;Performance improvement over baseline (%)&amp;quot;,
       caption = &amp;quot;Number of Trees ~ Number of Features&amp;quot;,
              title = &amp;quot;Xgboost via h2o: Performance over baseline&amp;quot;,
       subtitle = &amp;quot;10 model hyper-parameter grid search with 5 fold cross-validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-26-benchmarking-workstation-xgboost_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For 10 features, the results are very noisy with 4 cores being comparable until 10,000 trees are used. For both 100 and 1000 features, 16 (+32) cores are superior across the board with a near linear speed up as the number of samples increases. Whilst we might imagine that increasing core count from 4 to 16 should result in a near 4 times speed up (or a 75% improvment in performance), interestingly, we are only really see anywhere near this performance with over 50,000 rows, 10000 trees, and 1000 features. This is probably because &lt;code&gt;h2o&lt;/code&gt; is parallelised on the model level (- this is conjecture based on observing &lt;code&gt;htop&lt;/code&gt;), which means that for each fold of each model all the data has to be transferred between cores leading to a large overhead. In most of these test cases, the overhead of passing data and setting up jobs is taking up much of the potential benefit from additional cores. This leads to only a 2-3 times speed up. It’s likely that in larger data sets, with longer compute times, this would be less of an issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;In this post, I have looked at the performance of &lt;code&gt;xgboost&lt;/code&gt; via &lt;code&gt;h2o&lt;/code&gt; on a sample data set, using a real-world test case of a cross-validated grid search. I found that using the GPU resulted in slower run times across the board, although there was some indication that performance improved for larger data and more complex models. Increasing the physical CPU count to 16 increased performance up to a maximum of 70% over 4 cores (for 100,000 features, 1000 features and 10,000 trees) but adding virtual cores led to no benefit.&lt;/p&gt;
&lt;p&gt;A major takeaway for me is that I probably shouldn’t be relying on &lt;code&gt;h2o&lt;/code&gt; for my grid searching in future when using smaller data sets. Something to experiment with would be parallelising across the grid, with each model using a single core. Having very much swallowed the Kool-Aid when it comes GPU compute, I was also surprised by how poor the performance was here. This is something to test further as using &lt;code&gt;xgboost&lt;/code&gt; within &lt;code&gt;h2o&lt;/code&gt; makes it difficult to pick apart where the problem lies. A test with a larger data set would also be helpful, although this may take awhile to run!&lt;/p&gt;
&lt;p&gt;Any thoughts on these results would be appreciated, especially regarding the poor performance of the GPU. I am also in the market for a new ML toolbox. I’ve been looking at &lt;a href=&#34;https://github.com/mlr-org/mlr&#34;&gt;&lt;code&gt;mlr&lt;/code&gt;&lt;/a&gt; so any recommendations would be appreciated. I’ll be following this post up with another benchmarking post using &lt;code&gt;benchmarkme&lt;/code&gt; in the next few days - if I can resist turning off virtual cores and getting going with some more overclocking.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.7 now on CRAN - Tuberculosis reports and summary plots</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-7/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-7/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR 0.5.7&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update mainly focussed on building out new country level Tuberculosis (TB) report functionality but along the way this led to a new summary plotting function that quickly and easily shows TB trends across regions and globally. I also had some fun developing a hexsticker (Tweet at me with something you made using the package to get a physical version - whilst my postage money lasts…), reducing the dependencies with &lt;a href=&#34;https://github.com/jimhester/itdepends&#34;&gt;&lt;code&gt;itdepends&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://uptakeopensource.github.io/pkgnet/index.html&#34;&gt;&lt;code&gt;pkgnet&lt;/code&gt;&lt;/a&gt; and dealing with some breaking changes from an uncoming &lt;code&gt;dplyr&lt;/code&gt; update (my own fault for missing a function import).&lt;/p&gt;
&lt;p&gt;The full changelog is below along with an example of the country level TB report generated for the UK (generate a report on the country of your choice using &lt;code&gt;getTBinR::render_country_report(country = &amp;quot;United Kingdom&amp;quot;, save_dir = &amp;quot;.&amp;quot;)&lt;/code&gt;).&lt;/p&gt;
&lt;div id=&#34;feature-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added support for &lt;code&gt;annual_change&lt;/code&gt; to &lt;code&gt;summarise_tb_burden&lt;/code&gt; and added validating tests.&lt;/li&gt;
&lt;li&gt;Added support for rates and proportions to &lt;code&gt;summarise_tb_burden&lt;/code&gt; and added validating tests.&lt;/li&gt;
&lt;li&gt;Added a new function - &lt;code&gt;plot_tb_burden_summary&lt;/code&gt;. Function wraps &lt;code&gt;summarise_tb_burden&lt;/code&gt; and allows all in one summary plotting. Inspired by this case study.&lt;/li&gt;
&lt;li&gt;Added a rmarkdown parameterised country level report on TB (&lt;code&gt;render_country_report&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Added a report generating button to the dashboard generated by &lt;code&gt;run_tb_dashboard&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Tweaked &lt;code&gt;map_tb_burden&lt;/code&gt; to not use &lt;code&gt;geom_path&lt;/code&gt; for country outlines.&lt;/li&gt;
&lt;li&gt;Added a smooth argument to &lt;code&gt;plot_tb_burden&lt;/code&gt; to allow smooth trend lines to be plotted (derived using &lt;code&gt;ggplot2::geom_smooth&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Tweaked line thickness in &lt;code&gt;plot_tb_burden&lt;/code&gt; to improve plot appearance.&lt;/li&gt;
&lt;li&gt;Added legend argument to all plotting functions to allow control of the legend appearance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;package-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added script to generate hexsticker&lt;/li&gt;
&lt;li&gt;Added hexsticker to README&lt;/li&gt;
&lt;li&gt;Added DOI link to Zenodo.&lt;/li&gt;
&lt;li&gt;Updated tests to account for &lt;code&gt;dplyr&lt;/code&gt; 8.0 release and &lt;code&gt;vdiffr&lt;/code&gt; updates.&lt;/li&gt;
&lt;li&gt;Added itdepends to package report functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-united-kingdom-tuberculosis-report&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: United Kingdom Tuberculosis Report&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Load the package
library(getTBinR)
## Load additional packages
library(dplyr) # For data munging 
library(tidyr)
library(rlang)
library(ggplot2)

## Get the data
tb &amp;lt;- get_tb_burden(verbose = FALSE)
## Get the data dictionary
dict &amp;lt;- get_data_dict(verbose = FALSE)
##Assign parameters - these are set in the YAML within the package
country &amp;lt;- &amp;quot;United Kingdom&amp;quot;
interactive &amp;lt;- FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-incidence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB incidence rates&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metric_summary &amp;lt;- function(df = NULL, target_country = NULL, metric = NULL) {
  
  target_country &amp;lt;- df$country[grepl(target_country, df$country)] %&amp;gt;% 
    unique %&amp;gt;% 
    first
    
  ##  Set up metric with confidence intervals
  metric &amp;lt;- enquo(metric)
  metric_lo &amp;lt;- sym(paste0(quo_name(metric), &amp;quot;_lo&amp;quot;))
  metric_hi &amp;lt;- sym(paste0(quo_name(metric), &amp;quot;_hi&amp;quot;))
  
  ## Filter for the country of interest
  country_df &amp;lt;- df %&amp;gt;% 
  filter(country %in% target_country)
  
## Most up to date year of incidence data
recent_inc &amp;lt;- country_df %&amp;gt;% 
  drop_na(!!metric) %&amp;gt;% 
  filter(year == max(year)) %&amp;gt;% 
  select(!!metric, !!metric_lo, !!metric_hi, year, g_whoregion) %&amp;gt;% 
  mutate(inc_rate = paste0(!!metric, &amp;quot; (&amp;quot;, !!metric_lo, &amp;quot; - &amp;quot;, !!metric_hi, &amp;quot;)&amp;quot;))
## Country rank
ranked_countries_inc &amp;lt;- df %&amp;gt;% 
  filter(year == recent_inc$year) %&amp;gt;% 
  arrange(desc(!!metric)) %&amp;gt;% 
  mutate(rank = 1:n())
## World rank
target_rank_world &amp;lt;- ranked_countries_inc %&amp;gt;% 
  filter(country == target_country) %&amp;gt;% 
  pull(rank)
## Region rank
target_rank_region &amp;lt;- ranked_countries_inc %&amp;gt;% 
  filter(g_whoregion %in% recent_inc$g_whoregion) %&amp;gt;% 
  mutate(rank = 1:n()) %&amp;gt;% 
  filter(country == target_country) %&amp;gt;%
  pull(rank)
## Summarise annual change
country_change &amp;lt;- summarise_tb_burden(metric = quo_name(metric),
                                      stat = &amp;quot;mean&amp;quot;,
                                      countries = target_country,
                                      compare_to_region = FALSE,
                                      compare_to_world = FALSE,
                                      compare_all_regions = FALSE,
                                      annual_change = TRUE,
                                      verbose = FALSE) %&amp;gt;% 
  filter(year &amp;gt; (max(year) - 10)) %&amp;gt;% 
  summarise(change = mean(!!metric, na.rm = FALSE)) %&amp;gt;%
  mutate(change = round(change * 100, 1) %&amp;gt;% 
           paste0(., &amp;quot;%&amp;quot;)) %&amp;gt;% 
  pull(change)
out &amp;lt;- list(recent_inc$year[1], recent_inc$inc_rate[1],
            target_rank_world, target_rank_region,
            country_change)
names(out) &amp;lt;- c(&amp;quot;year&amp;quot;, &amp;quot;metric&amp;quot;, &amp;quot;world_rank&amp;quot;, &amp;quot;region_rank&amp;quot;, &amp;quot;avg_change&amp;quot;)
out &amp;lt;- ifelse(is.na(out), &amp;quot;(Missing)&amp;quot;, out)
return(out)
}
  
  
inc_sum &amp;lt;- metric_summary(tb, country, e_inc_100k)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis incidence rate of 8.9 (8.1 - 9.8) per 100,000 people making it number 165 in the world and number 32 regionally. In the last 10 years this has changed by -4.9% on average each year.&lt;/p&gt;
&lt;div id=&#34;regional-and-global-trends-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regional and Global Trends Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(countries = country,
                       metric_label = &amp;quot;e_inc_100k&amp;quot;,
                       compare_to_world = TRUE, 
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       annual_change = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;,
                       interactive = interactive,
                       verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rates Regional Breakdown&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(countries = country,
                        compare_to_region = TRUE,
                        interactive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-detection-rates-cdr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case Detection Rates (CDR)&lt;/h2&gt;
&lt;p&gt;United Kingdom had an estimated case detection rate of 89 (81 - 98)% in 2017 making it number 4 in the world (with number 1 having the highest CDR) and number 3 regionally. In the last 10 years this has changed by 0% on average each year.&lt;/p&gt;
&lt;div id=&#34;regional-breakdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;c_cdr&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-mortality-rates---excluding-hiv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB mortality rates - excluding HIV&lt;/h2&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis mortality rate (excluding HIV) of 0.53 (0.52 - 0.53) per 100,000 people making it number 166 in the world and number 32 regionally. In the last 10 years this has changed by -1.6% on average each year.&lt;/p&gt;
&lt;div id=&#34;proportion-of-tb-cases-that-died-excluding-hiv---regional-and-global-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Proportion of TB Cases that Died (excluding HIV) - Regional and Global Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(metric = &amp;quot;e_mort_exc_tbhiv_num&amp;quot;,
                       denom = &amp;quot;e_inc_num&amp;quot;,
                       rate_scale = 100,
                       countries = country,
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       interactive = interactive,
                       verbose = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;) +
  labs(y = &amp;quot;Proportion (%) of TB cases that died (excluding HIV)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rates Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;e_mort_exc_tbhiv_100k&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-hiv-related-mortality-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB HIV related mortality rates&lt;/h2&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis mortality rate (related to HIV) of 0.1 (0.05 - 0.16) per 100,000 people making it number 127 in the world and number 23 regionally. In the last 10 years this has changed by 7.6% on average each year.&lt;/p&gt;
&lt;div id=&#34;proportion-of-tb-cases-that-died-related-to-hiv---regional-and-global-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Proportion of TB Cases that Died (related to HIV) - Regional and Global Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(metric = &amp;quot;e_mort_tbhiv_num&amp;quot;,
                       denom = &amp;quot;e_inc_num&amp;quot;,
                       rate_scale = 100,
                       countries = country,
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       interactive = interactive,
                       verbose = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;) +
  labs(y = &amp;quot;Proportion (%) of TB cases that died (related to HIV)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rates Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;e_mort_tbhiv_100k&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building an Rstats Workstation</title>
      <link>http://www.samabbott.co.uk/post/building-an-rstats-workstation/</link>
      <pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/building-an-rstats-workstation/</guid>
      <description>


&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I regularly use cloud resources (AWS and GCP) both in my day job and for personal projects but recently I have been finding that having to spin up a cloud instance for quick analysis can be tedious, even when making use of tools for reproducibility like docker. This is particularly the case for self-learning when spending money on cloud resources feels wasteful, especially when I have half an eye on something else (i.e the TV). Often I find that this leads me to not look at the things that I am interested in, although this could also just be my own lack of motivation!&lt;/p&gt;
&lt;p&gt;It seemed sensible to finally bite the bullet and build a local workstation that could handle all the compute tasks I can throw at it (up to a point obviously). At the moment I am looking to further explore &lt;a href=&#34;http://www.stats.ox.ac.uk/%7Edoucet/andrieu_doucet_holenstein_PMCMC.pdf&#34;&gt;P-MCMC methods&lt;/a&gt;, Deep learning and get involved in a few &lt;a href=&#34;https://www.kaggle.com&#34;&gt;kaggle&lt;/a&gt; competitions. These needs inform my parts choices below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;requirements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A high core number for compute intensive CPU tasks that can easily be parallel enabled such as Sequential Monte Carlo and distributed machine learning (&lt;code&gt;xgboost&lt;/code&gt;, &lt;code&gt;h2o&lt;/code&gt; etc).&lt;/li&gt;
&lt;li&gt;Good single core CPU performance as many tasks are natively single core (i.e R).&lt;/li&gt;
&lt;li&gt;Enough RAM to support the CPU cores but this is an area that I could economise as more RAM can be added at a later date. Typically 2Gbs per core is the minimum.&lt;/li&gt;
&lt;li&gt;A GPU with good support and sufficient power to do meaningful deep learning. As of my current reading this essentially means a Nvidia GPU.&lt;/li&gt;
&lt;li&gt;A fast hard drive but a large one is not needed as only the data currently used in analysis needs to be stored (on top of the OS and applications).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;parts-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parts list&lt;/h2&gt;
&lt;p&gt;Knowing nothing about building PCs, or PC components, I started using a suggested build on &lt;a href=&#34;https://pcpartpicker.com/guide/NKV323/gaming-streaming-and-editing-build&#34;&gt;pcpartpicker&lt;/a&gt; for streaming and editing. This was a good match as editing/streaming require a larger core number than other use cases, whilst still needing a strong GPU. From here, I went through each part in turn and reviewed the alternatives. The main resources that I used for PC part reviews and thoughts were: the &lt;a href=&#34;https://thewirecutter.com&#34;&gt;thewirecutter&lt;/a&gt;, &lt;a href=&#34;https://www.anandtech.com&#34;&gt;anandtech&lt;/a&gt; and &lt;a href=&#34;https://www.tomshardware.com&#34;&gt;tomshardware&lt;/a&gt;. From this reading, I settled on a Threadripper CPU from the previous generation as this provided 16 cores, relatively good single core performance and some scope for further overclocking. Another viable option was the current generation Threadripper, which has improved speed and better automatic overclocking but ultimately I decided that the cost/benefit didn’t make sense. The choice of CPU dictated the motherboard choice, which I selected based on the parametric filters pcpartpicker provides (a real lifesaver). As the Threadrippers are very heat intensive I went with a water cooler for the CPU (&lt;em&gt;Note: I probably should have chosen a bigger radiator here for more cooling&lt;/em&gt;), again selected based on the parametric filters + reviews. I went with 32GB of 3200 Mhz RAM as the minimum required for this many CPU cores, with a good balance of speed and price. I chose the Nvidia RTX 2070 Windforce for GPU after looking at some benchmarks and because the 2080 was dramatically more expensive. The Windforce also comes with a slightly higher clock speed than other entry level 2070s along with 3 fans for additional cooling. Finally, I chose the best reviewed M2 SSD that I could find as the read/write speeds are dramatically faster than traditional SSDs, going with a smaller disk size (500 Gb) to minimise the cost. The final parts list is &lt;a href=&#34;https://uk.pcpartpicker.com/list/9FpFMZ&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2018-12-21-building-an-rstats-workstation/parts.png&#34; alt=&#34;All the parts (excluding the case) ready to go.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;All the parts (excluding the case) ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I was slightly nervous about the build process but after reading the manuals for all of the parts (quite dull - the motherboard and case manuals turned out to be the most helpful) and watching multiple youtube videos going through the PC building process (there are lots of great channels providing great resources) it went without a hitch. In brief the build process boiled down to the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installing the CPU into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the RAM into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the SSD into the motherboard (as using an M.2 SSD).&lt;/li&gt;
&lt;li&gt;Installing the motherboard into the case&lt;/li&gt;
&lt;li&gt;Adding the CPU cooler to the case&lt;/li&gt;
&lt;li&gt;Plugging the fans into the correct motherboard sockets&lt;/li&gt;
&lt;li&gt;Plugging the case ports into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the GPU into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the power supply and linking up the power connections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This all sounds very simple and, amazingly, it really was. After going through this process over a few hours (most of the time was spent looking for screws and getting confused over which instruction manual to use), all the parts were installed and everything powered up correctly (having RGB was very reassuring here).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;os&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;OS&lt;/h2&gt;
&lt;p&gt;Historically, I have been a Mac user but recently I have been using cloud resources and docker more and more. Both of these use Linux so this seemed like a sensible choice. Not knowing much about the various distros available I wanted to go with one based on Debian, as these use the same package libraries etc. that I have been using on AWS and elsewhere. I settled on &lt;a href=&#34;https://ubuntubudgie.org/downloads&#34;&gt;Ubuntu Budgie&lt;/a&gt;, which is a theme for standard Ubuntu (one of the most commonly used distros). Budgie provides a modern, lightweight interface, that overlays the core Ubuntu distro. I followed &lt;a href=&#34;https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu&#34;&gt;a guide&lt;/a&gt; for creating an OS USB boot and then followed the installation instructions after rebooting the workstation (&lt;em&gt;Note: During installation I chose to encrypt the boot disk. This means that the computer requires a password before booting - meaning that it can’t be remotely rebooted. I would definitely not choose to do this in the future.&lt;/em&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Software&lt;/h2&gt;
&lt;p&gt;After completing the Ubuntu set up and installing the usual suspects (i.e Dropbox etc.), my first requirement was something to reproduce the functionality of &lt;a href=&#34;https://www.alfredapp.com&#34;&gt;alfred&lt;/a&gt; (an amazing Mac only spotlight replacement that I completely rely on for navigating my computer). After a brief search, I came across &lt;a href=&#34;https://albertlauncher.github.io/docs/installing/&#34;&gt;albert&lt;/a&gt; a great open source Linux alternative. The next step was to get my most commonly used Linux command line tools, namely &lt;code&gt;htop&lt;/code&gt; (to monitor CPU usage) and &lt;code&gt;tree&lt;/code&gt; (to explore the file system). These were installed in the terminal with the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install htop tree&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that everything was working correctly in the build, I needed to be able to monitor CPU temperatures, both ideal and under load. I chose to do this using &lt;a href=&#34;https://github.com/amanusk/s-tui&#34;&gt;&lt;code&gt;s-tui&lt;/code&gt;&lt;/a&gt; - a great tool that monitors CPU usage, temperatures, power consumption and speed. It provides an interface to the &lt;code&gt;stress&lt;/code&gt; package, allowing load scenarios to be simulated. I also needed to be able to check that the GPU was working as expected. To do this, I used the following command in the terminal: &lt;code&gt;watch -n 5 nvidia-smi&lt;/code&gt; - this calls &lt;code&gt;nvidia-smi&lt;/code&gt; every 5 seconds. &lt;code&gt;nvidia-smi&lt;/code&gt; comes prepackaged with the Nvidia drivers (you may need to update/switch your drivers - I used &lt;a href=&#34;http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux&#34;&gt;this post&lt;/a&gt;) and allows monitoring of Nvidia GPUs (much like &lt;code&gt;top&lt;/code&gt; for CPUs).&lt;/p&gt;
&lt;p&gt;As this workstation is mostly going to be running analyses via &lt;a href=&#34;https://hub.docker.com/r/rocker/tidyverse&#34;&gt;rocker&lt;/a&gt;-based docker containers, the next key step is to download and install &lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1&#34;&gt;Docker CE&lt;/a&gt;. See the Docker documentation for more details on using docker (well worth the time as Docker is the one of the best tools for ensuring that analysis is reproducible). If everything is installed and working correctly, the following should lead to an instance of an Rstudio server at &lt;code&gt;localhost:8787&lt;/code&gt; with the username and password of &lt;code&gt;seabbs&lt;/code&gt; (you may need to use &lt;code&gt;sudo&lt;/code&gt; here).&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -d -p 8787:8787 -e USER=seabbs -e PASSWORD=seabbs --name rstudio rocker/tidyverse&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, you should have everything you need to do Rstats using your new workstation! The final software that is required is &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/a&gt;, this provides a docker wrapper that allows docker containers to access the Nvidia GPU. After installing, check its working using the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;overclocking-and-stress-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overclocking and stress tests&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2018-12-21-building-an-rstats-workstation/benchmarks.png&#34; alt=&#34;Stress testing the CPU and GPU using s-tui and ethminer&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Stress testing the CPU and GPU using &lt;code&gt;s-tui&lt;/code&gt; and &lt;code&gt;ethminer&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As the 1950X Threadripper has a relatively low clock speed of 3.4 Mhz, there is general agreement online that with proper cooling there is some good overhead for overclocking. Depending on the use case, this may or may not be worth doing as for mostly low core work flows the turbo-boosting (not available when overclocked) will give improved performance over most overclocks. In my case, I am most interested in optimizing multicore use and am willing to sacrifice a small boost in single core performance (of around 5%) to achieve this.&lt;/p&gt;
&lt;p&gt;In order to check that an overclock is both stable and does not increase the temperature of the system to dangerous levels, it is important to stress test both the CPU and GPU over an extended time period (I initially started with an hour and then extended this to 6 hours for the final test). To do this I used &lt;code&gt;s-tui&lt;/code&gt; to both monitor and stress the CPU, &lt;a href=&#34;https://github.com/ethereum-mining/ethminer&#34;&gt;&lt;code&gt;ethminer&lt;/code&gt;&lt;/a&gt; to stress the GPU by mining Ethereum and &lt;code&gt;nvidia-smi&lt;/code&gt; to monitor the GPU temperature. To overclock the system, I made &lt;a href=&#34;https://overclocking.guide/gigabyte-threadripper-overclocking-guide/&#34;&gt;adjustments in the BIOS&lt;/a&gt;, starting with the RAM (to get it running at its maximum specifications, which is not the default), and then moving to increasing the CPU speed by 100 Mhz intervals each time. Following this process, I got to a speed of 3.8 Mhz (see the picture above for the stress test at this level). Going higher than this required an increase in voltage for stability and this lead to dramatic temperature increases (beyond 70C). A possible option is turning off simultaneous multithreading (i.e going from 32 virtual cores to 16 real cores), this would reduce power consumption and allow for a higher overclock at the cost of reduced potential parallelisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;static-ip-and-remote-access&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Static IP and Remote Access&lt;/h1&gt;
&lt;p&gt;This workstation will primarily be used headlessly from my Macbook Pro so the next important step is to set up remote access. I generally like to connect over SSH but sometimes it is very useful to have an GUI interface to work with. To allow this I installed a VNC server (&lt;a href=&#34;https://www.realvnc.com/en/&#34;&gt;RealVNC VNC Server&lt;/a&gt;). The nice thing about this is that it works out of the box both on the local network and externally. For the SSH setup, I hardened my configuration using &lt;a href=&#34;https://help.ubuntu.com/community/SSH/OpenSSH/Configuring&#34;&gt;this&lt;/a&gt; post (I also changed my SSH port from 22 to something else to limit automatic detection). After following these steps, the workstation now needs the public keys of any computers that you want to connect to it using SSH - some tips for this can be found &lt;a href=&#34;https://help.ubuntu.com/community/SSH/OpenSSH/Keys&#34;&gt;here&lt;/a&gt;. To make connections to the workstation consistent it needs a static IP. I did this using the &lt;a href=&#34;https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux&#34;&gt;Ubuntu interface&lt;/a&gt; rather than using my router as I usually would and it was so easy that I will definitely favour this option in the future. The next step is to allow SSH connection from your computer when your not at home. To do this you need to open a port in your router and link it to the SSH port of the workstation (see the instructions for your router). If, like me, you don’t have a static IP address then you need to find a workaround. I used &lt;a href=&#34;https://www.noip.com&#34;&gt;noip&lt;/a&gt; to provide an address to SSH to rather than an IP. Finally, I like to add aliases to avoid having to write out the IP each time I want to SSH. Below I have an example setup (this needs to be copied into the &lt;code&gt;.profile&lt;/code&gt; file).&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias archie_internal=&amp;#39;ssh -p 2345 user@your-local-ip&amp;#39;
alias archie=&amp;#39;ssh -p 2345 user@computer.hopto.org&amp;#39;
alias archie_with_ports=&amp;#39;ssh -p 2345 -l localhost:8787:localhost:8787 user@computer.hopto.org&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test your connection from the connecting computer using the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;archie_with_ports&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should be able to control the workstation in the terminal and see Rstudio at &lt;code&gt;localhost:8787&lt;/code&gt; (This post was written remotely using this approach and the &lt;code&gt;seabbs/seabbs.github.io&lt;/code&gt; docker container).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;p&gt;Whilst I now have a working workstation there are still some things that need sorting out. The most important of these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work out how to use GPUs in arbitrary docker containers without first installing cuda etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Suggestions for how best to use &lt;a href=&#34;https://twitter.com/nvidia?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@nvidia&lt;/span&gt;&lt;/a&gt; GPUs in rocker &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/docker?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#docker&lt;/a&gt; containers. Ideally looking for a solution using nvidia docker compose rather than manually installing cuda into each container? Does this exist or are there good alternatives?
&lt;/p&gt;
— Sam Abbott (&lt;span class=&#34;citation&#34;&gt;@seabbs&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/seabbs/status/1075709270979686401?ref_src=twsrc%5Etfw&#34;&gt;December 20, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Benchmark GPU vs CPU for common tasks such as training &lt;a href=&#34;https://xgboost.ai&#34;&gt;Xgboost models&lt;/a&gt; and using &lt;a href=&#34;https://xgboost.ai&#34;&gt;libBi&lt;/a&gt; (for P-MCMC and SMC-SMC).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explore whether simultaneous multi-threading (hyper-threading) leads to performance improvements in common Rstats work loads. For Intel CPUs, my experience is that only using real cores leads to better performance for intensive compute tasks. If this is the case here then this will allow for greater overclocking headroom.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;things-to-improve-next-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Things to improve next time&lt;/h1&gt;
&lt;p&gt;This was a huge learning experience (which I really enjoyed). My main takeaways so far have been:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Think hard about parts trade-off&lt;/strong&gt; - for example, I went with the last generation Threadripper as a cost saving initiative. In retrospect, the latest generation might have been worth the money because the new automatic overclocking features mean that significant improves are likely for most workloads.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Look carefully at cooling options&lt;/strong&gt; - for example, a larger radiator would give more head room from CPU overclocking.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrap Up&lt;/h1&gt;
&lt;p&gt;If you got through this post then thanks for reading! Hopefully it gave some insights into how to approach building out a workstation for Rstats. I am definitely not an expert so any thoughts would be very welcome. As mentioned above, I am particularly interested in performance comparisons between GPUs and CPUs (both real and virtual cores) and so would welcome any insights into this aspect of things.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.5 now on CRAN - 2017 data.</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-5/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-5/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR 0.5.5&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update is mainly about highlighting the availability of TB data for 2017, although some small behind the scenes changes were required to get the code set up going forward for yearly updates. A few more plotting options have been added, along with the corresponding tests (definitely the most exciting news). The full changelog is below along with a short example highlighting some of the changes in the 2017 data.&lt;/p&gt;

&lt;p&gt;The main message from the 2017 data is that in 2017 there were again over 10 million estimated TB cases globally with only a 1.8% decrease in incidence rates compared to 2016. Over the last 10 years progress has been made with an average of a 1.9% decrease in TB incidence rates year on year. However there is little evidence of an increase in the rate that TB incidence rates are falling, with incidence rates forecast to remain at over 100 per 100,000 for the next 10 years if progress is made at the same rate as in the last decade.&lt;/p&gt;

&lt;h2 id=&#34;feature-updates&#34;&gt;Feature updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added a years filter to &lt;code&gt;plot_tb_burden&lt;/code&gt; and &lt;code&gt;plot_tb_burden_overview&lt;/code&gt;. This allows a range of years to be plotted. The default is all years which was the previous de facto default.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;package-updates&#34;&gt;Package updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Updated docs to reflect new year of data.&lt;/li&gt;
&lt;li&gt;Updated examples to use the new year of data as standard.&lt;/li&gt;
&lt;li&gt;Updated README to always use the current year of data.&lt;/li&gt;
&lt;li&gt;Updated all vignettes to reflect new data or be fixed to historic data as appropriate.&lt;/li&gt;
&lt;li&gt;Update site with links out to blog posts using the newest version of &lt;a href=&#34;http://pkgdown.r-lib.org&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pkgdown&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example-changes-in-tb-incidence-rates-in-2017&#34;&gt;Example: Changes in TB incidence rates in 2017&lt;/h2&gt;

&lt;p&gt;The code below quickly explores the updated data by first estimating global incidence rates and the annual change between years. The country level annual changes in TB incidence rates are then plotted, first globally and then by region. Finally, the trend in incidence rates is explored using country, regional and global level TB incidence rates. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-5-5.png&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;ggrepel&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;)

##Pull TB data 
tb_burden &amp;lt;- get_tb_burden() 


## Summarise global changes
global_tb &amp;lt;- tb %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  summarise_at(.vars = c(&amp;quot;e_inc_num&amp;quot;, &amp;quot;e_pop_num&amp;quot;), ~ sum(., na.rm = TRUE)) %&amp;gt;% 
  mutate(inc_rate = e_inc_num / e_pop_num * 1e5,
         per_change_inc = (inc_rate - lag(inc_rate)) / lag(inc_rate)) %&amp;gt;% 
  mutate(g_whoregion = &amp;quot;Global&amp;quot;,
         label = ifelse(year == max(year), g_whoregion, &amp;quot;&amp;quot;))



global_tb

## TB in 2017
tb_2017 &amp;lt;- global_tb %&amp;gt;% 
  filter(year == 2017)

tb_2017

## Global annual change
global_annual_change &amp;lt;- ggplot(global_tb, aes(year, per_change_inc)) +
  geom_smooth(se = FALSE, col = &amp;quot;black&amp;quot;, size = 1.2, alpha.line = 0.7) +
  geom_point(size = 1.2, alpha = 0.8, col = &amp;quot;black&amp;quot;) +
  scale_y_continuous(label = scales::percent, minor_breaks = NULL, breaks = seq(-0.025, 0, 0.0025)) +
  theme_minimal() +
  labs(
    y = &amp;quot;Annual Percentage Change&amp;quot;,
    x = &amp;quot;Year&amp;quot;,
    title = &amp;quot;Global Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
    caption = &amp;quot;&amp;quot;
    
  )

global_annual_change

## Remove countries with incidence below 1000 or incidence rates below 10 per 100,000 to reduce noise and cal country level annual change.
countries_with_tb_burden &amp;lt;- tb_burden %&amp;gt;% 
  filter(year == 2017,
         e_inc_100k &amp;gt; 10,
         e_inc_num &amp;gt; 1000)

tb_annual_change &amp;lt;- tb_burden %&amp;gt;% 
  semi_join(countries_with_tb_burden, by = &amp;quot;country&amp;quot;) %&amp;gt;% 
  group_by(country) %&amp;gt;% 
  arrange(year) %&amp;gt;% 
  select(year, g_whoregion, country, e_inc_100k, e_inc_num, e_pop_num) %&amp;gt;% 
  mutate(annual_change = (e_inc_100k - lag(e_inc_100k)) / lag(e_inc_100k)) %&amp;gt;% 
  ungroup

## Function to plot annual change
plot_annual_change &amp;lt;- function(df, strat = NULL, subtitle = NULL, years = 2000:2017) {
  dist &amp;lt;- df %&amp;gt;% 
    filter(year %in% years) %&amp;gt;% 
    rename(Region = g_whoregion) %&amp;gt;% 
    mutate(year = year %&amp;gt;% 
             factor(ordered = TRUE) %&amp;gt;% 
             fct_rev) %&amp;gt;% 
    ggplot(aes_string(x = &amp;quot;annual_change&amp;quot;, y = &amp;quot;year&amp;quot;, col = strat, fill = strat)) +
    geom_density_ridges(quantile_lines = TRUE, quantiles = 2, alpha = 0.6) +
    scale_color_viridis(discrete = TRUE, end = 0.9) +
    scale_fill_viridis(discrete = TRUE, end = 0.9) +
    geom_vline(xintercept = 0, linetype = 2, alpha = 0.6) +
    scale_x_continuous(labels = scales::percent, breaks = seq(-0.4, 0.4, 0.1),
                       limits = c(-0.4, 0.4), minor_breaks = NULL) +
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(x = paste0(&amp;quot;Annual Change in &amp;quot;, search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition),
         y = &amp;quot;Year&amp;quot;,
         title = &amp;quot;Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
         subtitle = subtitle,
         caption = &amp;quot;&amp;quot;)
  
  return(dist)
}

## Overall country level annual change
overall &amp;lt;- plot_annual_change(tb_annual_change, NULL,
                              years = seq(2001, 2017, 2), subtitle = &amp;quot;By Country&amp;quot;) 

overall

## Regional country level annual change
region &amp;lt;-  plot_annual_change(tb_annual_change, &amp;quot;Region&amp;quot;,
                              subtitle = &amp;quot;By Region&amp;quot;, 
                              years = seq(2001, 2017, 2)) + 
  facet_wrap(~Region) +
  labs(caption = &amp;quot;&amp;quot;)

region

## Regional and Global TB incidence rates over time
regional_incidence &amp;lt;- tb_burden %&amp;gt;% 
  group_by(g_whoregion, year) %&amp;gt;% 
  summarise(inc = sum(e_inc_num, na.rm = TRUE), pop = sum(e_pop_num, na.rm = TRUE)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(inc_rate = inc / pop * 1e5) %&amp;gt;% 
  mutate(label = ifelse(year == max(year), g_whoregion, &amp;quot;&amp;quot;)) %&amp;gt;% 
  ggplot(aes(year, inc_rate, col = g_whoregion)) +
  geom_line(alpha = 0.8, size = 1.2) +
  scale_color_viridis(discrete = TRUE, end = 0.9) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_y_continuous(breaks = seq(0, 400, 25), minor_breaks = NULL, limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(2000, 2017, 1), minor_breaks = NULL) +
  geom_text_repel(aes(label = label),
                  nudge_x = 4,
                  force = 10,
                  na.rm = TRUE,
                  min.segment.length = 10) +
  labs(x = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       y = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
       subtitle = &amp;quot;By Region, per 100,000 population&amp;quot;) +
  geom_line(data = global_tb, aes(col = NULL), alpha = 0.8, size = 1.2) +
  geom_text_repel(data = global_tb,
                  aes(label = label, col = NULL),
                  nudge_x = 2,
                  nudge_y = 8,
                  na.rm = TRUE)

## Map global TB incidence rates for 2017 using getTBinR
map &amp;lt;- map_tb_burden(year = c(2005, 2009, 2013, 2017), facet = &amp;quot;year&amp;quot;) +
  theme(strip.background = element_blank()) +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
       subtitle = &amp;quot;By Country, per 100,000 population&amp;quot;)

## Compose storyboard
storyboard &amp;lt;- (map + regional_incidence + plot_layout(widths = c(2, 1))) /
  (region + (global_annual_change /
               overall + labs(caption = &amp;quot;For country level annual percentages change countries with incidence above 1000 and an incidence rate above 10 per 100,000 are shown.
                    The global annual percentage change is shown with a LOESS fit. 
                    By @seabbs | Made with getTBinR | Source: World Health Organisation&amp;quot;)) + plot_layout(widths = c(2, 1))) +
  plot_layout(widths = c(1, 1))

## Save storyboard
ggsave(&amp;quot;storyboard.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/getTBinR/storyboard-5-5.png&#34; alt=&#34;getTBinR 0.5.5 storyboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34; target=&#34;_blank&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34; target=&#34;_blank&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.4 now on CRAN - new data, map updates and a new summary function.</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-4/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-4/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR 0.5.4&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update includes an additional data set for 2016 containing variables related to drug resistant Tuberculosis, some aesthetic updates to mapping functionality and a new &lt;code&gt;summarise_tb_burden&lt;/code&gt; function for summarising TB metrics. Behind the scenes there has been an extensive test overhaul, with &lt;a href=&#34;https://github.com/lionel-/vdiffr&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;vdiffr&lt;/code&gt;&lt;/a&gt; being used to test images, and several bugs fixes. See below for a full list of changes and some example code exploring the new functionality.&lt;/p&gt;

&lt;h2 id=&#34;feature-updates&#34;&gt;Feature updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added MDR-TB data for 2016, see &lt;a href=&#34;http://www.who.int/tb/country/data/download/en/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the dataset. The MDR-TB data is automatically joined to the WHO TB burden data.&lt;/li&gt;
&lt;li&gt;Aesthetic updates to &lt;code&gt;map_tb_burden&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Added new &lt;code&gt;summarise_tb_burden&lt;/code&gt; function for summarising metrics across regions, across custom groups and globally.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;package-updates&#34;&gt;Package updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improved data cleaning, converting &lt;code&gt;Inf&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt; values to &lt;code&gt;NA&lt;/code&gt; when the data is read in.&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;pgknet&lt;/code&gt; report.&lt;/li&gt;
&lt;li&gt;Improved test robustness and scope&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;vdiffr&lt;/code&gt; to test plots when not on CRAN.&lt;/li&gt;
&lt;li&gt;Fixed bug for &lt;code&gt;map_tb_burden&lt;/code&gt; which was adding duplicate variables which caused map build to fail.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example-exploring-global-tb-incidence-rates&#34;&gt;Example: Exploring Global TB Incidence Rates&lt;/h2&gt;

&lt;p&gt;For a quick example the code below pulls the WHO TB data, summarises it by region using &lt;code&gt;getTBinR&lt;/code&gt; and &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;ggridges&lt;/code&gt;&lt;/a&gt;, maps global TB incidence rates in 2016, and finally plots an overview of incidence rates in the 10 countries with highest TB incidence rates in 2016. The figures generated are then combined into a single storyboard using the &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pathwork&lt;/code&gt;&lt;/a&gt; package. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-5-4.png&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;)

## Pull TB data and summarise TB incidence rates by region using the median
tb_sum &amp;lt;- summarise_tb_burden(metric = &amp;quot;e_inc_100k&amp;quot;,
                              stat = &amp;quot;median&amp;quot;,
                              compare_all_regions = TRUE,
                              samples = 1000)

## Plot the median and IQR for each region
sum &amp;lt;- tb_sum %&amp;gt;% 
  rename(Region = area) %&amp;gt;% 
  ggplot(aes(x = year, y = e_inc_100k, col = Region, fill = Region)) +
  geom_ribbon(alpha = 0.2, aes(ymin = e_inc_100k_lo, ymax = e_inc_100k_hi)) +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  geom_line(alpha = 0.6, size = 1.2) +
  geom_point(size = 1.3) +
  theme_minimal() +
  facet_wrap(~Region, scales = &amp;quot;free_y&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(y = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       x = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Regional Summary of Tuberculosis Incidence Rates - 2000 to 2016&amp;quot;,
       subtitle = &amp;quot;Median country level incidence rates (with 95% interquartile ranges) are shown&amp;quot;)

## Get the full TB burden dataset (including MDR TB)
tb &amp;lt;- get_tb_burden()

## Plot the distribution of country level TB incidence rates using ggridges
dist &amp;lt;- tb %&amp;gt;% 
  rename(Region = g_whoregion) %&amp;gt;% 
  mutate(year = year %&amp;gt;% 
           factor(ordered = TRUE) %&amp;gt;% 
           fct_rev) %&amp;gt;% 
  ggplot(aes(x = e_inc_100k, y = year, col = Region, fill = Region)) +
  geom_density_ridges(alpha = 0.6) +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  theme_minimal() +
  facet_wrap(~Region, scales = &amp;quot;free_x&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(x = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       y = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Distribution of Country Level Tuberculosis Incidence Rates by Region - 2000 to 2016&amp;quot;,
       caption = &amp;quot;By @seabbs | Made with getTBinR | Source: World Health Organisation&amp;quot;)

## Map global TB incidence rates for 2016 using getTBinR
map &amp;lt;- map_tb_burden() +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;Map of Tuberculosis Incidence Rates - 2016&amp;quot;)

## Extract the top 10 high incidence countries in 2016.
high_inc_countries &amp;lt;- tb %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  arrange(desc(e_inc_100k)) %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  pull(country)

## Plot an overview of TB incidence rates in 2016.
high_inc_overview &amp;lt;- plot_tb_burden_overview(countries = high_inc_countries) +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;10 Countries with the Highest Tuberculosis Incidence Rates - 2016&amp;quot;) 

## Compose storyboard
storyboard &amp;lt;- (map + high_inc_overview) /
                 (sum | dist) +
                 plot_layout(heights = c(1, 2))

## Save storyboard
ggsave(&amp;quot;storyboard.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/getTBinR/storyboard-5-4.png&#34; alt=&#34;getTBinR 0.5.4 storyboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34; target=&#34;_blank&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34; target=&#34;_blank&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
